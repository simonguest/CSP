---
title: "Exploring Generative AI Models: Part 2"
author: "Simon Guest"
format:
  revealjs:
    slide-number: true
    incremental: true
    center-title-slide: true
    theme: default
logo: ../../src/logos/DigiPen_RGB_Red.png
bibliography: references.bib
---

## Recap of Last Week's Lecture

- TBD

## Images Models: Text-to-Image

Text-to-Image: "A photograph of an astronaut riding a horse."

- Based on a concept called a **diffuser**
- Two stage process, inspired by thermodynamics

## Image Models: Diffuser

- Training
  - During training, random noise is adding to images in steps
  - Model learns to predict what noise was added (forward diffusion process)
- Inference (process runs in reverse)
  - Start with pure random noise (known as a seed)
  - Model estimates what noise should be removed to create a realistic image
  - Using the text prompt, the model steers the process towards images that match the description

## Image Diffusion Models in 2022

```{mermaid}
timeline
  August 2022 : Stable Diffusion v1.4
              : First open-source high-quality model
  September 2022 : Stable Diffusion v1.5
                  : Refined version
  October 2022 : eDiff-I (NVIDIA)
                : Ensemble approach
  November 2022 : Stable Diffusion v2.0/2.1
                : Higher resolution (768x768)
```

## Image Diffusion Models in 2023

```{mermaid}
timeline
  March 2023 : Midjourney v5
              : Exceptional artistic quality
  April 2023 : ControlNet
        : Precise spatial control
        : AnimateDiff - Video generation
  July 2023 : SDXL (Stable Diffusion XL)
            : 1024x1024 native resolution
  August 2023 : SDXL Turbo
              : Real-time capable generation
```

## Image Diffusion Models in 2024

```{mermaid}
timeline
  February 2024 : Sora (OpenAI)
                : Text-to-video up to 60 seconds
                : Stable Diffusion 3
                : Improved text understanding
  June 2024 : Stable Diffusion 3.5
            : Multiple model sizes
  2024 : FLUX.1 (Black Forest Labs)
        : State-of-the-art open model
        : Imagen 3 (Google DeepMind)
        : Photorealistic quality
```

## Demo: Text to Image Diffusion Process {.center}

[Diffusion Text-to-Image.ipynb](https://colab.research.google.com/drive/1YZYskU2laocx2dNpyYSjcTZZuVQBb4xX?usp=sharing){.external target="_blank"}

## Images Models: Image-to-Image

Image-to-Image: "Make this image different"

- Originally solved by GAN approaches, but evolved into extension of the diffusion concept
  - Add noise to the original image (partial denoising)
  - Regenerate it with modifications based on the prompt
  - The original image heavily influences the output structure

## Demo: Image to Image Process {.center}

[Diffusion Image-to-Image.ipynb](https://colab.research.google.com/drive/1MaQ-WvYOVF_wb-HpQNNsVxemG4xt6YGV?usp=sharing){.external target="_blank"}

## ControlNet

Text-to-Image and Image-to-Image are interesting, but you don't have that much control

- Extensive prompts (both positive and negative) can help, but only so far
- Fine-tuning techniques, but expensive and risk degrading quality

## ControlNet

Introducing ControlNet

- Developed by Lvmin Zhang and Maneesh Agrawala at Stanford University
- Published in February 2023 [@zhang2023controlnet]
- ControlNet represented a paradigm shift from "describe what you want" to "show the structure you want."

## How ControlNet Works

- Stable Diffusion's U-Net has an encoder and decoder
- Create a trainable copy of the encoder blocks
- Train the copy of the encoder alongside the frozen SD model
  - During training: use paired data (e.g., pose skeleton â†’ original image)
  - During inference: both encoders run together
  - Features from both are combined via zero convolutions
- Key: The weights in the original SD model don't change
- ControlNet is analogous to a "Plug in" model

## Demo: ControlNet Human Pose {.center}

[ControlNet Human Pose.ipynb](https://colab.research.google.com/drive/1uW5ix9dnHTMsoeUt38H936zYw5eYUk3u?usp=sharing){.external target="_blank"}

## ControlNet

Examples of conditioning types:

- Human pose (OpenPose) - skeleton/keypoint detection
- QR codes - codes blended into images
- Canny edges - line drawings and edge detection
- Depth maps - 3D structure information
- Normal maps - surface orientation
- Semantic segmentation - labeled regions
- Scribbles - rough user drawings

# Further Exploration

For the many image models that we didn't cover...

## Hugging Face Image Models

![Source: https://huggingface.co/models](./images/imagemodels.png)

## Replicate

![Source: https://replicate.com](./images/replicate.png)

## ComfyUI

![Source: https://comfy.org](./images/comfyui.png)

# Resources

## Resources

- This slide deck, resources, links, everything: 
  - [https://simonguest.github.io/CSP-400](https://simonguest.github.io/CSP-400){.external target="_blank"}
  - (I'll also post to the GAM-400 and CSP-300/400 channels)

# Q&A

# Bibliography