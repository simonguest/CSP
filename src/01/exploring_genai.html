<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <title>CSP-400 – Exploring Generative AI Models</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-534cd8e3a96973385dffff3f4709048d.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Exploring Generative AI Models</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="recap" class="slide level2">
<h2>Recap</h2>
<ul>
<li class="fragment">Last Week’s Lecture
<ul>
<li class="fragment">Introduced AI Agents, their uses, how to create</li>
<li class="fragment">About 50% had used some kind of API</li>
<li class="fragment">One or two beyond this</li>
</ul></li>
<li class="fragment">This Week
<ul>
<li class="fragment">Explore text and image-based models</li>
<li class="fragment">Model evolution, API access, running locally</li>
<li class="fragment">Demos!</li>
</ul></li>
</ul>
</section>
<section>
<section id="a-brief-history-of-transformer-models" class="title-slide slide level1 center">
<h1>A Brief History of Transformer Models</h1>

</section>
<section id="a-brief-history-of-transformer-models-1" class="slide level2">
<h2>A Brief History of Transformer Models</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
    June 2017 : Google researchers publish "Attention is all you need" paper
              : Introduces self-attention mechanism and transformer architecture
              : Eliminates the need for recurrent neural networks in sequence processing
    
    June 2018 : OpenAI releases GPT-1
              : 117M parameters
              : Demonstrates pre-training on large text corpora followed by fine-tuning works effectively
    
    Feb 2019 : OpenAI releases GPT-2
             : 1.5B parameters
             : Initially withheld full model due to concerns about misuse
             : Demonstrates impressive text generation capabilities with minimal fine-tuning
    
    May 2020 : OpenAI releases GPT-3
             : 175B parameters
             : Demonstrates strong few-shot learning capabilities
             : Marks a significant leap in model capabilities and scale
    
    June 2020 : GPT-3 available through OpenAI API
              : Still a completion model, not instruction-tuned
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="completion-vs.-instruction-tuned" class="slide level2">
<h2>Completion vs.&nbsp;Instruction-Tuned</h2>
<ul>
<li class="fragment">Completion Model just predicts the next token
<ul>
<li class="fragment">Input prompt - <code>Mary had a little</code></li>
<li class="fragment">Max total tokens - <code>50</code></li>
<li class="fragment">Temperature - <code>0 - 1.0</code></li>
<li class="fragment">top_k - consider only the top k tokens in the response</li>
<li class="fragment">top_p - Nucleus sampling (probability cut off - 0 and 1.0)</li>
</ul></li>
<li class="fragment">Output
<ul>
<li class="fragment"><code>Mary had a little lamb, its fleece was white as snow...</code> (up to max tokens)</li>
</ul></li>
</ul>
</section>
<section id="completion-vs.-instruction-tuned-1" class="slide level2">
<h2>Completion vs.&nbsp;Instruction-Tuned</h2>
<ul>
<li class="fragment">You can’t really converse with it</li>
<li class="fragment"><code>What is the capital of France?</code> (max tokens = 50)</li>
<li class="fragment"><code>What is the capital of France? Paris. What is the capital of Spain? Madrid. What is the capital of</code></li>
</ul>
</section>
<section id="introducing-google-colab" class="slide level2">
<h2>Introducing Google Colab</h2>

<img data-src="./images/colab-signup.png" class="r-stretch quarto-figure-center"><p class="caption">Source: https://colab.research.google.com/signup</p></section></section>
<section>
<section id="demo-gpt-2" class="title-slide slide level1 center">
<h1>Demo: GPT-2</h1>
<p><a href="https://colab.research.google.com/drive/1H0NZYU-U3BmTS2bUqrVw9ssuFsp5mr8n?usp=sharing" class="external" target="_blank">GPT-2.ipynb</a></p>
</section>
<section id="instruction-tuned-models" class="slide level2">
<h2>Instruction-Tuned Models</h2>
<ul>
<li class="fragment">Supervised Fine-Tuning
<ul>
<li class="fragment">Large datasets of questions/answers, tasks/completions, demonstrating helpful assistant behavior</li>
</ul></li>
<li class="fragment">RLHF (Reinforcement Learning from Human Feedback)
<ul>
<li class="fragment">Human raters rank different model responses, training a reward model</li>
</ul></li>
<li class="fragment">Chat Templates
<ul>
<li class="fragment">Structured formats to distinguish speakers in a dialog: Typically system, user, and assistant</li>
</ul></li>
</ul>
</section>
<section id="a-brief-history-of-transformer-models-2" class="slide level2">
<h2>A Brief History of Transformer Models</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
    2021 : InstructGPT Development
          : Built on GPT-3 with RLHF fine-tuning
          : Trained to follow instructions and understand user intent
          : Key innovation enabling ChatGPT
    
    Jan 2021 : Anthropic Founded
             : Founded by Dario &amp; Daniela Amodei with ~7 senior OpenAI employees
            : Dario led GPT-2/3 development and co-invented RLHF

    Nov 2022 : ChatGPT Launch
                  : Built on GPT-3.5 using RLHF
                  : 1M+ users in 5 days
                  : Sparked widespread interest in generative AI

    Feb 2023 : Llama 1 Released
                  : Meta's LLaMA (7B, 13B, 33B, 65B parameters)
                  : 13B model exceeded GPT-3 (175B) on most benchmarks
                  : Limited researcher access
                  : Text completion only (Alpaca fine-tune added instructions)

    Jul 2023 : Llama 2 Released
              : Available in 7B, 13B, 70B sizes
              : Trained on 40% more data than Llama 1
              : First open-weights Llama for commercial use
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="closed-vs.-open-models" class="slide level2">
<h2>Closed vs.&nbsp;Open Models</h2>
<ul>
<li class="fragment"><strong>Closed Source:</strong> Hosted models; no ability to inspect the weights of the models. Accessed via an API (or UI).
<ul>
<li class="fragment">Examples: OpenAI GPT-5, Claude Sonnet 4.5</li>
</ul></li>
<li class="fragment"><strong>Open Weight:</strong> Model files with pretrained weights, but no training data. Host on your own hardware.
<ul>
<li class="fragment">Examples: Meta’s Llama (and derivatives), Mistral</li>
</ul></li>
<li class="fragment"><strong>Open Source Models:</strong> Models with access to the training data set. Create from scratch.
<ul>
<li class="fragment">Examples: OLMo from AI2</li>
</ul></li>
</ul>
</section>
<section id="accessing-models-via-apis" class="slide level2">
<h2>Accessing Models via APIs</h2>
<ul>
<li class="fragment">HTTP-based APIs
<ul>
<li class="fragment">Client makes HTTP API calls to invoke/access the model</li>
<li class="fragment">(Normally use an SDK to wrap the HTTP API calls)</li>
<li class="fragment">Client passes Authorization token as part of the call</li>
<li class="fragment">Default way of accesing OpenAI, Claude, other large, closed-source models</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="demo-openai-sdkapi-call" class="title-slide slide level1 center">
<h1>Demo: OpenAI SDK/API call</h1>
<p><a href="https://colab.research.google.com/drive/1ktzdxJUcHQ6yEY8tJSeikTXjVgwG9DLc?usp=sharing" class="external" target="_blank">OpenAI.ipynb</a></p>
</section>
<section id="openais-chat-completions-api" class="slide level2">
<h2>OpenAI’s Chat Completions API</h2>
<ul>
<li class="fragment">Debuted in March 2023, together with the ChatGPT API</li>
<li class="fragment">Structure
<ul>
<li class="fragment">Messages array (system, assistant, user)</li>
<li class="fragment">Streaming support (using SSE)</li>
<li class="fragment">Simple parameters</li>
<li class="fragment">Function calling (added mid-2023)</li>
</ul></li>
<li class="fragment">Widespread Adoption
<ul>
<li class="fragment">Langchain, other SDKs</li>
<li class="fragment">Anthropic, Azure, TogetherAI</li>
<li class="fragment">Local hosting: vLLM, LM Studio</li>
</ul></li>
</ul>
</section>
<section id="introducing-openrouter" class="slide level2">
<h2>Introducing OpenRouter</h2>
<p>How about creating a client that consumes different models from multiple providers?</p>
<ul>
<li class="fragment">Introducing OpenRouter (https://openrouter.ai)
<ul>
<li class="fragment">A unified API to hundreds of AI models through a single endpoint</li>
<li class="fragment">OpenAI, Claude, Gemini, Grok, Nova, Llama, DeepSeek, Qwen and many others.</li>
<li class="fragment">OpenAI Chat Completions compatible</li>
</ul></li>
</ul>
</section>
<section id="demo-openrouter" class="slide level2">
<h2>Demo: OpenRouter</h2>
<p><a href="https://colab.research.google.com/drive/16HB_vtFl5QkSrkGhb5onBgC0Cjx59K9S?usp=sharing" class="external" target="_blank">OpenRouter.ipynb</a></p>
</section>
<section id="downloading-and-running-models" class="slide level2">
<h2>Downloading and Running Models</h2>
<ul>
<li class="fragment">So far, we’ve called hosted models via APIs</li>
<li class="fragment">How about downloading and running models on your own hardware?
<ul>
<li class="fragment">(Obviously they need to be open-weight models)</li>
</ul></li>
</ul>
</section>
<section id="downloading-and-running-models-1" class="slide level2">
<h2>Downloading and Running Models</h2>
<p>Why would you want to do this?</p>
<ul>
<li class="fragment">Offline access to models (no Internet required)</li>
<li class="fragment">Potential cost savings (if many API calls and already own hardware)
<ul>
<li class="fragment">e.g., running a small model embedded within a game</li>
</ul></li>
<li class="fragment">Want to fine-tune your own model and have the hardware to do it</li>
<li class="fragment">Don’t want others to see what types of conversations you are having :)</li>
</ul>
</section>
<section id="introducing-hugging-face" class="slide level2">
<h2>Introducing Hugging Face</h2>

<img data-src="./images/huggingface.png" class="r-stretch quarto-figure-center"><p class="caption">Source: https://huggingface.co</p></section>
<section id="what-is-hugging-face" class="slide level2">
<h2>What is Hugging Face?</h2>
<p>It is to AI models what GitHub is to source code</p>
<ul>
<li class="fragment">Explore, download models to run on local hardware</li>
<li class="fragment">Upload and share your own trained/fine-tuned models and datasets</li>
<li class="fragment">Create “Spaces” - web-based apps for accessing models</li>
</ul>
</section>
<section id="demo-exploring-a-model-on-hugging-face" class="slide level2">
<h2>Demo: Exploring a Model on Hugging Face</h2>
<p><a href="https://huggingface.co/google/gemma-3-1b-it" class="external" target="_blank">Google’s gemma-3-1b-it on Hugging Face</a></p>
</section>
<section id="hugging-face-transformers" class="slide level2">
<h2>Hugging Face Transformers</h2>

<img data-src="./images/transformers.png" class="r-stretch quarto-figure-center"><p class="caption">Source: https://huggingface.co/docs/transformers</p></section>
<section id="hugging-face-transformers-1" class="slide level2">
<h2>Hugging Face Transformers</h2>
<p>What is the Hugging Face Transformers Library?</p>
<ul>
<li class="fragment">Open-source Python library to provide easy access to using various types of pre-trained transformer models</li>
<li class="fragment">Brings together all of the different formats under one interface.
<ul>
<li class="fragment">Different models, vendors, types, chat templates</li>
<li class="fragment">Different implementations: PyTorch, TensorFlow, JAX</li>
</ul></li>
<li class="fragment">A few lines of code to download and run the model</li>
</ul>
</section>
<section id="demo-using-transformers-lib-to-download-and-use-gemma-3-1b" class="slide level2">
<h2>Demo: Using Transformers Lib to download and use Gemma 3 1B</h2>
<p><a href="https://colab.research.google.com/drive/1-2515Fku_5vtgUx7zRF0o0QdZgOTGZk2?usp=sharing" class="external" target="_blank">gemma-3-1b-it via transformers.ipynb</a></p>
</section>
<section id="vram-envy" class="slide level2">
<h2>VRAM Envy</h2>
<p>One challenge of running moels on your own hardware is VRAM availability!</p>
<ul>
<li class="fragment">Roughly speaking, the size of the model will determine how much VRAM you need</li>
<li class="fragment">Gemma 3 models
<ul>
<li class="fragment">gemma-3-1b-it = 2Gb</li>
<li class="fragment">gemma-3-4b-it = 8.6Gb</li>
<li class="fragment">gemma-3-12b-it = 23.37Gb</li>
</ul></li>
<li class="fragment">Colab Tiers
<ul>
<li class="fragment">Colab Free T4 = 16Gb VRAM (15Gb usable)</li>
<li class="fragment">Colab Pro V100 = 16Gb VRAM</li>
<li class="fragment">Colab Pro A100 = 40Gb VRAM</li>
</ul></li>
<li class="fragment">Your Hardware
<ul>
<li class="fragment">Probably 8Gb VRAM :)</li>
</ul></li>
</ul>
</section>
<section id="vram-envy-1" class="slide level2">
<h2>VRAM Envy</h2>
<p>You can select smaller models, but they are less accurate / more prone to hallucination.</p>
<ul>
<li class="fragment">How do we fix this?
<ul>
<li class="fragment">Quantization</li>
</ul></li>
</ul>
</section>
<section id="quantization" class="slide level2">
<h2>Quantization</h2>
<p>Process of reducing the precision of a model’s weights and activations. For example, 16-bit numbers to 4-bit.</p>
<ul>
<li class="fragment">Parameter count matters more than precision
<ul>
<li class="fragment">A 70B parameter model at 4-bit often beats a 13B model at b16</li>
<li class="fragment">The models knowledge remains largely intact</li>
<li class="fragment">Often the extra precision doesn’t meaningfully improve outputs</li>
</ul></li>
</ul>
</section>
<section id="quantization-formats" class="slide level2">
<h2>Quantization Formats</h2>
<p>The llama.cpp project (implementing LLMs in pure C/C++) has driven advancements in quantization</p>
<ul>
<li class="fragment">GGUF (GPT-Generated Unifed Format)
<ul>
<li class="fragment">Single file architecture</li>
<li class="fragment">Model format supporting multiple quantization levels (2-bit through 8-bit)</li>
</ul></li>
<li class="fragment">MLX (Apple’s ML framework and format for Apple Silicon)
<ul>
<li class="fragment">Debuted in late 2023</li>
<li class="fragment">Supports 4 and 8 bit quantization schemes</li>
</ul></li>
<li class="fragment">Tools built upon llama.cpp
<ul>
<li class="fragment">Ollama, LM Studio, koboldcpp</li>
</ul></li>
</ul>
</section></section>
<section id="demo-running-gemma-3-27b-gguf-on-my-laptop" class="title-slide slide level1 center">
<h1>Demo: Running Gemma 3 27B GGUF on my laptop</h1>
<ul>
<li class="fragment">LMStudio: https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf</li>
<li class="fragment">55Gb at b16, 16Gb quantized</li>
</ul>
</section>

<section>
<section id="going-beyond-text" class="title-slide slide level1 center">
<h1>Going Beyond Text</h1>

</section>
<section id="images-models---generation" class="slide level2">
<h2>Images Models - Generation</h2>
<p>Text-to-Image: “A photograph of an astronaut riding a horse.”</p>
<ul>
<li class="fragment">Based on a concept called a diffuser</li>
<li class="fragment">Two stage process, inspired by thermodynamics</li>
<li class="fragment">Training
<ul>
<li class="fragment">During training, random noise is adding to images in steps</li>
<li class="fragment">Model learns to predict what noise what added (forward diffusion process)</li>
</ul></li>
<li class="fragment">Inference
<ul>
<li class="fragment">To generate new images, the process runs in reverse</li>
<li class="fragment">Start with pure random noise (known as a seed)</li>
<li class="fragment">Model estimates what noise should be removed to create a realistic image</li>
<li class="fragment">Using the text prompt, the model steers the process towards images that match the description</li>
</ul></li>
</ul>
<p>TBD: add sources (don’t forget sources throughout)</p>
<p>2015 - Deep Unsupervised Learning using Nonequilibrium Thermodynamics (Sohl-Dickstein et al.) 2019 - Generative Modeling by Estimating Gradients of the Data Distribution (Song &amp; Ermon) 2020 - Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., UC Berkeley)</p>
</section>
<section id="image-diffusion-models-in-2022" class="slide level2">
<h2>Image Diffusion Models in 2022</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
  August 2022 : Stable Diffusion v1.4
              : First open-source high-quality model
  September 2022 : Stable Diffusion v1.5
                  : Refined version
  October 2022 : eDiff-I (NVIDIA)
                : Ensemble approach
  November 2022 : Stable Diffusion v2.0/2.1
                : Higher resolution (768x768)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="image-diffusion-models-in-2023" class="slide level2">
<h2>Image Diffusion Models in 2023</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
  March 2023 : Midjourney v5
              : Exceptional artistic quality
  April 2023 : ControlNet
        : Precise spatial control
        : AnimateDiff - Video generation
  July 2023 : SDXL (Stable Diffusion XL)
            : 1024x1024 native resolution
  August 2023 : SDXL Turbo
              : Real-time capable generation
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="image-diffusion-models-in-2024" class="slide level2">
<h2>Image Diffusion Models in 2024</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
  February 2024 : Sora (OpenAI)
                : Text-to-video up to 60 seconds
                : Stable Diffusion 3
                : Improved text understanding
  June 2024 : Stable Diffusion 3.5
            : Multiple model sizes
  2024 : FLUX.1 (Black Forest Labs)
        : State-of-the-art open model
        : Imagen 3 (Google DeepMind)
        : Photorealistic quality
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section></section>
<section>
<section id="demo-text-to-image-diffusion-process" class="title-slide slide level1 center">
<h1>Demo: Text to Image Diffusion Process</h1>
<p><a href="https://colab.research.google.com/drive/1YZYskU2laocx2dNpyYSjcTZZuVQBb4xX?usp=sharing" class="external" target="_blank">Diffusion Text-to-Image.ipynb</a></p>
</section>
<section id="image-models---generation" class="slide level2">
<h2>Image Models - Generation</h2>
<p>Image-to-Image: “Make this image different”</p>
<ul>
<li class="fragment">Originally solved by GAN approaches, but evolved into extension of the diffusion concept
<ul>
<li class="fragment">Add noise to the original image (partial denoising)</li>
<li class="fragment">Regenerate it with modifications based on the prompt</li>
<li class="fragment">The original image heavily influences the output structure</li>
</ul></li>
</ul>
<p>Stable Diffusion with img2img (2022) - Popularized diffusion-based image-to-image for the masses. It works by encoding the input image to latent space, adding noise, then denoising with text conditioning. The “strength” parameter controls how much the original image influences the result.</p>
</section></section>
<section>
<section id="demo-image-to-image-process" class="title-slide slide level1 center">
<h1>Demo: Image to Image Process</h1>
<p><a href="https://colab.research.google.com/drive/1MaQ-WvYOVF_wb-HpQNNsVxemG4xt6YGV?usp=sharing" class="external" target="_blank">Diffusion Image-to-Image.ipynb</a></p>
</section>
<section id="controlnet" class="slide level2">
<h2>ControlNet</h2>
<p>Text-to-Image and Image-to-Image are interesting, but you don’t really have that much control.</p>
<ul>
<li class="fragment">Fine-tuning techniques, but expensive and risk degrading quality</li>
</ul>
</section>
<section id="controlnet-1" class="slide level2">
<h2>ControlNet</h2>
<p>Introducing ControlNet</p>
<ul>
<li class="fragment">Developed by Lvmin Zhang and Maneesh Agrawala at Stanford University</li>
<li class="fragment">Published in February 2023 in the paper “Adding Conditional Control to Text-to-Image Diffusion Models.”</li>
<li class="fragment">ControlNet represented a paradigm shift from “describe what you want” to “show the structure you want.”</li>
</ul>
</section>
<section id="how-controlnet-works" class="slide level2">
<h2>How ControlNet Works</h2>
<ul>
<li class="fragment">Stable Diffusion’s U-Net has an encoder and decoder</li>
<li class="fragment">Create a trainable copy of the encoder blocks</li>
<li class="fragment">Train the copy of the encoder alongside the frozen SD model
<ul>
<li class="fragment">During training: use paired data (e.g., pose skeleton → original image)</li>
<li class="fragment">During inference: both encoders run together - original processes noisy latent, copy processes control input</li>
<li class="fragment">Features from both are combined via zero convolutions</li>
</ul></li>
<li class="fragment">Key: The weights in the original SD model doesn’t change</li>
<li class="fragment">Analogous to ControlNet being a “Plug in” model</li>
</ul>
</section></section>
<section>
<section id="demo-controlnet-human-pose" class="title-slide slide level1 center">
<h1>Demo: ControlNet Human Pose</h1>
<p><a href="https://colab.research.google.com/drive/1uW5ix9dnHTMsoeUt38H936zYw5eYUk3u?usp=sharing" class="external" target="_blank">ControlNet Human Pose.ipynb</a></p>
</section>
<section id="controlnet-2" class="slide level2">
<h2>ControlNet</h2>
<p>Exmaples of conditioning types</p>
<ul>
<li class="fragment">Canny edges - line drawings and edge detection</li>
<li class="fragment">Depth maps - 3D structure information</li>
<li class="fragment">Normal maps - surface orientation</li>
<li class="fragment">Human pose (OpenPose) - skeleton/keypoint detection</li>
<li class="fragment">Semantic segmentation - labeled regions</li>
<li class="fragment">Scribbles - rough user drawings</li>
</ul>
</section>
<section id="controlnet-based-methods" class="slide level2">
<h2>ControlNet-Based Methods</h2>
<ul>
<li class="fragment">Advanced Editing Techniques
<ul>
<li class="fragment">Scribble to image / QR code to image</li>
<li class="fragment">Inpainting (better at large variations vs.&nbsp;other inpainting techniques).</li>
<li class="fragment">Image Alteration - e.g., Weather and Environmental Enhancement</li>
</ul></li>
<li class="fragment">Structural and Geometric Transformation
<ul>
<li class="fragment">Pose Estimation</li>
<li class="fragment">Normal Map Generation</li>
<li class="fragment">Sematic Segmentation</li>
</ul></li>
<li class="fragment">Neural Style Transfer
<ul>
<li class="fragment">Ghibli images, anyone?</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="further-tools-to-explore" class="title-slide slide level1 center">
<h1>Further Tools To Explore</h1>
<p>For the many image models that we didn’t cover…</p>
</section>
<section id="replicate" class="slide level2">
<h2>Replicate</h2>

<img data-src="./images/replicate.png" class="r-stretch quarto-figure-center"><p class="caption">Source: https://replicate.com</p></section>
<section id="comfyui" class="slide level2">
<h2>ComfyUI</h2>

<img data-src="./images/comfyui.png" class="r-stretch quarto-figure-center"><p class="caption">Source: https://comfy.org</p></section>
<section id="resources" class="slide level2">
<h2>Resources</h2>
<p>TBD</p>
</section>
<section id="before-we-go" class="slide level2">
<h2>Before we go…</h2>
<p>How about the other way?</p>
<ul>
<li class="fragment">Classic models are typically supervised classification tasks
<ul>
<li class="fragment">“Is this a cat?”</li>
<li class="fragment">End up taking lots of pictures of cats, human annotation, etc.</li>
</ul></li>
<li class="fragment">Introducing the Vision Transformer (ViT)
<ul>
<li class="fragment">“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”</li>
<li class="fragment">More on this in the next lecture!</li>
</ul></li>
</ul>
</section></section>
<section id="qa" class="title-slide slide level1 center">
<h1>Q&amp;A</h1>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../src/logos/DigiPen_RGB_Red.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/simonguest\.github\.io\/CSP-400\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>