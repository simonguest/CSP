[
  {
    "objectID": "src/networking/websockets.html",
    "href": "src/networking/websockets.html",
    "title": "WebSockets",
    "section": "",
    "text": "WebSockets is a communication protocol that enables full-duplex, bidirectional interaction between a client (such as a web browser) and a server over a single, long-lived connection. Unlike traditional HTTP, which follows a request-response model, WebSockets establish a persistent channel that allows both parties to send and receive data in real time without repeatedly opening new connections. This makes it highly efficient for scenarios where low latency and continuous updates are critical, such as chat applications, live notifications, multiplayer games, and real-time data streaming.\n\n\n\n\n\nflowchart LR\n    subgraph Client[\"Client (e.g., Browser, Mobile App)\"]\n        C1[WebSocket Client Library]\n        C2[Application Logic]\n        C2 --&gt; C1\n    end\n\n    subgraph Server[\"Server\"]\n        S1[WebSocket Server Endpoint]\n        S2[Application Logic / Handlers]\n        S3[Database / External APIs]\n        S2 --&gt; S3\n        S1 --&gt; S2\n    end\n\n    C1 &lt;--&gt; |Persistent WebSocket Connection| S1\n\n    %% Styling for readability\n    style Client fill:#f0f9ff,stroke:#0369a1,stroke-width:2px,rounded-corners:5px\n    style Server fill:#fef9c3,stroke:#ca8a04,stroke-width:2px,rounded-corners:5px\n\n\n\n\n\n\n\n\nThe most popular WebSockets library for Python is¬†websockets, a lightweight, easy-to-use, and widely adopted library that provides both client and server implementations conforming to the WebSocket protocol. It is built on top of Python‚Äôs¬†asyncio, making it well-suited for asynchronous, real-time applications, and is often recommended as the go-to choice in the Python ecosystem for WebSocket development.\n\n\n\n\nScalability Challenges¬†‚Äì Since WebSockets keep long-lived persistent connections, managing thousands or millions of concurrent clients requires careful scaling strategies (load balancing, connection sharding, etc.).\nServer Resource Usage¬†‚Äì Each connection consumes memory and file descriptors on the server, so inefficient handling can lead to resource exhaustion.\nComplexity Compared to HTTP¬†‚Äì WebSockets don‚Äôt have the same mature ecosystem of caching, load balancing, or monitoring that HTTP-based systems do, so debugging and managing them can be harder.\nFirewall and Proxy Compatibility¬†‚Äì Some firewalls, proxies, and enterprise networks may block or disrupt WebSocket traffic since it breaks the traditional request/response model.\nStatelessness Loss¬†‚Äì Unlike HTTP, WebSockets create stateful, continuous connections, which can complicate server restarts, failover, or horizontal scaling.\nLimited Built-in Features¬†‚Äì The protocol is low-level; things like authentication, reconnection logic, and message acknowledgment need to be implemented by the developer (or use a higher-level abstraction like¬†Socket.IO).",
    "crumbs": [
      "Networking Protocols",
      "WebSockets"
    ]
  },
  {
    "objectID": "src/networking/websockets.html#python-sdk",
    "href": "src/networking/websockets.html#python-sdk",
    "title": "WebSockets",
    "section": "",
    "text": "The most popular WebSockets library for Python is¬†websockets, a lightweight, easy-to-use, and widely adopted library that provides both client and server implementations conforming to the WebSocket protocol. It is built on top of Python‚Äôs¬†asyncio, making it well-suited for asynchronous, real-time applications, and is often recommended as the go-to choice in the Python ecosystem for WebSocket development.",
    "crumbs": [
      "Networking Protocols",
      "WebSockets"
    ]
  },
  {
    "objectID": "src/networking/websockets.html#limitations",
    "href": "src/networking/websockets.html#limitations",
    "title": "WebSockets",
    "section": "",
    "text": "Scalability Challenges¬†‚Äì Since WebSockets keep long-lived persistent connections, managing thousands or millions of concurrent clients requires careful scaling strategies (load balancing, connection sharding, etc.).\nServer Resource Usage¬†‚Äì Each connection consumes memory and file descriptors on the server, so inefficient handling can lead to resource exhaustion.\nComplexity Compared to HTTP¬†‚Äì WebSockets don‚Äôt have the same mature ecosystem of caching, load balancing, or monitoring that HTTP-based systems do, so debugging and managing them can be harder.\nFirewall and Proxy Compatibility¬†‚Äì Some firewalls, proxies, and enterprise networks may block or disrupt WebSocket traffic since it breaks the traditional request/response model.\nStatelessness Loss¬†‚Äì Unlike HTTP, WebSockets create stateful, continuous connections, which can complicate server restarts, failover, or horizontal scaling.\nLimited Built-in Features¬†‚Äì The protocol is low-level; things like authentication, reconnection logic, and message acknowledgment need to be implemented by the developer (or use a higher-level abstraction like¬†Socket.IO).",
    "crumbs": [
      "Networking Protocols",
      "WebSockets"
    ]
  },
  {
    "objectID": "src/networking/webrtc-sfu.html",
    "href": "src/networking/webrtc-sfu.html",
    "title": "WebRTC using an SFU (Selective Forwarding Unit)",
    "section": "",
    "text": "This shows how you scale beyond the pure peer-to-peer model (where each participant connects to every other participant). SFUs relay streams between peers, reducing load on clients.\n\n\n\n\n\nflowchart LR\n    subgraph Client1[\"Client 1\"]\n        C1A[WebRTC API]\n        C1B[App Logic]\n        C1B --&gt; C1A\n    end\n\n    subgraph Client2[\"Client 2\"]\n        C2A[WebRTC API]\n        C2B[App Logic]\n        C2B --&gt; C2A\n    end\n\n    subgraph Client3[\"Client 3\"]\n        C3A[WebRTC API]\n        C3B[App Logic]\n        C3B --&gt; C3A\n    end\n\n    subgraph SFU[\"SFU (Selective Forwarding Unit)\"]\n        S1[Media Router / Forwarder]\n    end\n\n    subgraph Signaling[\"Signaling Server\"]\n        Sig[Coordinate Session Setup]\n    end\n\n    subgraph STUN_TURN[\"STUN / TURN Servers\"]\n        N1[NAT Traversal / Relay if Needed]\n    end\n\n    %% Connections\n    C1A &lt;--&gt; |\"Signaling\"| Sig\n    C2A &lt;--&gt; |\"Signaling\"| Sig\n    C3A &lt;--&gt; |\"Signaling\"| Sig\n\n    C1A &lt;--&gt; |\"Media / Data\"| S1\n    C2A &lt;--&gt; |\"Media / Data\"| S1\n    C3A &lt;--&gt; |\"Media / Data\"| S1\n\n    C1A &lt;--&gt; N1\n    C2A &lt;--&gt; N1\n    C3A &lt;--&gt; N1\n\n    %% Styles\n    style Client1 fill:#f0f9ff,stroke:#0369a1,stroke-width:2px,rounded-corners:5px\n    style Client2 fill:#ede9fe,stroke:#6d28d9,stroke-width:2px,rounded-corners:5px\n    style Client3 fill:#fff7ed,stroke:#c2410c,stroke-width:2px,rounded-corners:5px\n    style SFU fill:#fde68a,stroke:#b45309,stroke-width:2px,rounded-corners:5px\n    style Signaling fill:#fef9c3,stroke:#ca8a04,stroke-width:2px,rounded-corners:5px\n    style STUN_TURN fill:#dcfce7,stroke:#15803d,stroke-width:2px,rounded-corners:5px\n\n\n\n\n\n\n\n\n\nClients (1,2,3‚Ä¶)¬†send their audio/video streams to the¬†SFU.\nThe¬†SFU¬†forwards each stream to the other clients, without mixing them (lightweight compared to MCU mixing).\nSignaling server¬†is still used for session setup (e.g., exchanging SDP & ICE candidates).\nSTUN/TURN servers¬†may be needed for NAT traversal or relaying in restrictive networks.\n\n\n\n\nEach client¬†uploads their media once¬†(instead of N times in pure P2P).\nSFU handles redistribution, keeping CPU/bandwidth low for clients.\nWorks well for¬†group calls, webinars, multi-user streaming.",
    "crumbs": [
      "Networking Protocols",
      "WebRTC using an SFU"
    ]
  },
  {
    "objectID": "src/networking/webrtc-sfu.html#sfu-based-webrtc-architecture-scalability",
    "href": "src/networking/webrtc-sfu.html#sfu-based-webrtc-architecture-scalability",
    "title": "WebRTC using an SFU (Selective Forwarding Unit)",
    "section": "",
    "text": "Clients (1,2,3‚Ä¶)¬†send their audio/video streams to the¬†SFU.\nThe¬†SFU¬†forwards each stream to the other clients, without mixing them (lightweight compared to MCU mixing).\nSignaling server¬†is still used for session setup (e.g., exchanging SDP & ICE candidates).\nSTUN/TURN servers¬†may be needed for NAT traversal or relaying in restrictive networks.\n\n\n\n\nEach client¬†uploads their media once¬†(instead of N times in pure P2P).\nSFU handles redistribution, keeping CPU/bandwidth low for clients.\nWorks well for¬†group calls, webinars, multi-user streaming.",
    "crumbs": [
      "Networking Protocols",
      "WebRTC using an SFU"
    ]
  },
  {
    "objectID": "src/01/resources.html",
    "href": "src/01/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This document contains all the external resources, links, and references mentioned in the ‚ÄúExploring Generative AI Models‚Äù lecture.",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html",
    "href": "src/00/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This document contains all the external resources, links, and references mentioned in the ‚ÄúIntroducing AI Agents‚Äù lecture.",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#demo-agent",
    "href": "src/00/resources.html#demo-agent",
    "title": "Resources",
    "section": "Demo Agent",
    "text": "Demo Agent\n\nCampus Agent Demo: https://simonguest-campus-agent.hf.space/\n\nInteractive demo of the DigiPen Campus Assistant agent\n\nCampus Agent Source Code: https://github.com/simonguest/CSP-400/blob/main/demos/00/campus-agent\n\nComplete source code for the DigiPen Campus Assistant demo",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#development-tools-and-platforms",
    "href": "src/00/resources.html#development-tools-and-platforms",
    "title": "Resources",
    "section": "Development Tools and Platforms",
    "text": "Development Tools and Platforms\n\nUI Frameworks\n\nGradio: https://www.gradio.app/\n\nPython library for building machine learning web interfaces\n\nGradio Documentation: https://gradio.app\n\nOfficial Gradio documentation and tutorials\n\nHugging Face Spaces: https://huggingface.co/spaces\n\nHost and share Gradio-based apps on Hugging Face\n\n\n\n\nAgent Frameworks\n\nOpenAI\n\nOpenAI Agents SDK (Python): https://openai.github.io/openai-agents-python/\n\nOfficial OpenAI agents framework for Python\n\nOpenAI Agents Visualization: https://openai.github.io/openai-agents-python/visualization/#showing-the-graph\n\nTool for visualizing agent graphs and interactions\n\n\n\n\nLangChain\n\nLangGraph: https://langchain-ai.github.io/langgraph/\n\nPython framework for building stateful, multi-actor applications with LLMs\n\n\n\n\nCrew AI\n\nCrew.ai: https://github.com/crewaiinc/crewai\n\nFramework for orchestrating role-playing, autonomous AI agents\n\n\n\n\nMicrosoft\n\nAutoGen: https://microsoft.github.io/autogen/stable/\n\nMicrosoft‚Äôs framework for building conversational AI systems\n\nMicrosoft Semantic Kernel: https://github.com/microsoft/semantic-kernel\n\nSDK for integrating AI services with conventional programming languages",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#protocols-and-standards",
    "href": "src/00/resources.html#protocols-and-standards",
    "title": "Resources",
    "section": "Protocols and Standards",
    "text": "Protocols and Standards\n\nModel Context Protocol (MCP): https://modelcontextprototcol.io\n\nStandardized protocol for connecting AI models with external tools and data sources",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#license-information",
    "href": "src/00/resources.html#license-information",
    "title": "Resources",
    "section": "License Information",
    "text": "License Information\nMost of the agent frameworks mentioned use the MIT License, making them suitable for both academic and commercial use: - OpenAI Agents SDK - LangGraph - Crew.ai - Microsoft AutoGen - Microsoft Semantic Kernel",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#sidebars",
    "href": "src/00/resources.html#sidebars",
    "title": "Resources",
    "section": "Sidebars",
    "text": "Sidebars\n\nSimple Made Easy: https://www.youtube.com/watch?v=SxdOUGdseq4\n\nRich Hickey‚Äôs talk on the difference between ‚Äúeasy‚Äù and ‚Äúsimple‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#articles-and-industry-reports",
    "href": "src/00/resources.html#articles-and-industry-reports",
    "title": "Resources",
    "section": "Articles and Industry Reports",
    "text": "Articles and Industry Reports\n\nWorld Economic Forum - Cognitive Enterprise: https://www.weforum.org/stories/2025/06/cognitive-enterprise-agentic-business-revolution/\n\nArticle on the agentic business revolution\n\nCRN - Hottest Agentic AI Tools: https://www.crn.com/news/ai/2025/10-hottest-agentic-ai-tools-and-agents-of-2025-so-far\n\nOverview of the top agentic AI tools of 2025\n\nGartner Press Release: https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027\n\nGartner‚Äôs predictions about agentic AI project success rates\n\nAnthropic - Building Effective Agents: https://www.anthropic.com/engineering/building-effective-agents\n\nEngineering guide on building effective AI agents and patterns\n\nE2B - AI Agents Landscape: https://e2b.dev\n\nOverview of the AI agents ecosystem and available frameworks",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSP-400",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CSP-400 course.",
    "crumbs": [
      "CSP-400"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "CSP-400",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CSP-400 course.",
    "crumbs": [
      "CSP-400"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#before-we-get-started",
    "href": "src/00/ai_agents.html#before-we-get-started",
    "title": "Introducing AI Agents",
    "section": "Before We Get Started",
    "text": "Before We Get Started\n\nHow many have used ChatGPT/Claude?\nHow many have built their own chatbot and/or used the OpenAI/Claude APIs?\nHow many have created something beyond this?",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#chatbots-are-not-that-smart",
    "href": "src/00/ai_agents.html#chatbots-are-not-that-smart",
    "title": "Introducing AI Agents",
    "section": "Chatbots are not that Smart",
    "text": "Chatbots are not that Smart\nThere are many limitations‚Ä¶\n\nNeeds constant human input every turn; No ability to plan beyond a single interaction\nSingle model with single context\nNo interaction with external systems",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent",
    "href": "src/00/ai_agents.html#what-is-an-agent",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.youtube.com/watch?v=bwXaJXgezf4",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent-1",
    "href": "src/00/ai_agents.html#what-is-an-agent-1",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.weforum.org/stories/2025/06/cognitive-enterprise-agentic-business-revolution/",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent-2",
    "href": "src/00/ai_agents.html#what-is-an-agent-2",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.crn.com/news/ai/2025/10-hottest-agentic-ai-tools-and-agents-of-2025-so-far",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent-3",
    "href": "src/00/ai_agents.html#what-is-an-agent-3",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent-4",
    "href": "src/00/ai_agents.html#what-is-an-agent-4",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\nImagine a DigiPen Campus Assistant: An agent that can help you navigate anything and everything at DigiPen!\n\n‚ÄúWhere can I find the ‚ÄòHopper‚Äô room?‚Äù\n‚ÄúCan you tell me more about FLM201?‚Äù\n‚ÄúOh, and what‚Äôs today‚Äôs vegetarian option at the Bytes Cafe?‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Planners\n\n\nAgents are driven by goals\nAnd they can put together a plan for the steps to complete that goal.\n\n‚ÄúFirst, I will discover where course information is located‚Äù\n‚ÄúThen I will search for any courses that reference FLM201‚Äù\n‚ÄúThen I summarize all of the key points for the student‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents-1",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents-1",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Autonomous\n\n\nAgents can then go off and execute the plan, independent of human input\nThe concept of ‚Äúhuman in the loop‚Äù still applies for confirmation\n\ne.g.¬†‚ÄúDo you really want to place this order at the Bytes Cafe?‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents-2",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents-2",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Reactive\n\n\nAgents can change mid-course depending on what they find and/or the environment.\n\ne.g.¬†‚ÄúI couldn‚Äôt find any course information on FLM201. I‚Äôm going to whether there are other 200-level FLM courses before responding to the student.‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents-3",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents-3",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents have Persistence\n\n\nAgents often have memory systems beyond the current conversation\nBroadly classified as short and long term memory\n\nShort term memory could be the options at the Bytes Cafe\nLong term memory could be your food preferences",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents-4",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents-4",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Interactive\n\n\nAgents can delegate to other agents for complex tasks\n\n(Or for tasks where other agents are better suited for.)\ne.g., Campus Agent -&gt; delegating to a Course Agent\n\nAgents can also be given access to external tools\n\ne.g., File search, Web search, access to the Bytes Cafe API",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#lets-try-this-1",
    "href": "src/00/ai_agents.html#lets-try-this-1",
    "title": "Introducing AI Agents",
    "section": "Let‚Äôs Try This!",
    "text": "Let‚Äôs Try This!\n\n\n\n\nhttps://simonguest-campus-agent.hf.space/",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#lets-try-this-2",
    "href": "src/00/ai_agents.html#lets-try-this-2",
    "title": "Introducing AI Agents",
    "section": "Let‚Äôs Try This!",
    "text": "Let‚Äôs Try This!\n\n\n\n\nhttps://simonguest-campus-agent.hf.space/",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#observations-and-questions",
    "href": "src/00/ai_agents.html#observations-and-questions",
    "title": "Introducing AI Agents",
    "section": "Observations and Questions",
    "text": "Observations and Questions\nGet into groups of 2 or 3\n\nQ: What worked? What surprised you?\nQ: What didn‚Äôt work? Where did the agent fail?\nQ: What other examples of agents can you think of?",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#other-examples-for-agents",
    "href": "src/00/ai_agents.html#other-examples-for-agents",
    "title": "Introducing AI Agents",
    "section": "Other Examples for Agents",
    "text": "Other Examples for Agents\n\nCustomer service agent\nTravel booking agent\nResearch assistant\nCode generation agent (very popular right now)\nAgents within games\n\nTraditional NPCs are pre-written dialogue trees\nWhereas agents can be more independent within the game environment",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#gradio-based-ui",
    "href": "src/00/ai_agents.html#gradio-based-ui",
    "title": "Introducing AI Agents",
    "section": "Gradio-Based UI",
    "text": "Gradio-Based UI\n\nSource: https://www.gradio.app/",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#gradio-based-ui-1",
    "href": "src/00/ai_agents.html#gradio-based-ui-1",
    "title": "Introducing AI Agents",
    "section": "Gradio-Based UI",
    "text": "Gradio-Based UI\nimport time\nimport gradio as gr\n\ndef slow_echo(message, history):\n    for i in range(len(message)):\n        time.sleep(0.05)\n        yield \"You typed: \" + message[: i + 1]\n\ndemo = gr.ChatInterface(\n    slow_echo,\n    flagging_mode=\"manual\",\n    flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n    save_history=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#gradio-based-ui-2",
    "href": "src/00/ai_agents.html#gradio-based-ui-2",
    "title": "Introducing AI Agents",
    "section": "Gradio-Based UI",
    "text": "Gradio-Based UI\n\nSource: https://gradio.app",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#hosted-on-hugging-face-spaces",
    "href": "src/00/ai_agents.html#hosted-on-hugging-face-spaces",
    "title": "Introducing AI Agents",
    "section": "Hosted on Hugging Face Spaces",
    "text": "Hosted on Hugging Face Spaces\n\nSource: https://huggingface.co/spaces",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#agent-structure",
    "href": "src/00/ai_agents.html#agent-structure",
    "title": "Introducing AI Agents",
    "section": "Agent Structure",
    "text": "Agent Structure\n\nAgents and tools for the DigiPen Campus Assistant",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#so-many-agent-frameworks",
    "href": "src/00/ai_agents.html#so-many-agent-frameworks",
    "title": "Introducing AI Agents",
    "section": "So Many Agent Frameworks‚Ä¶",
    "text": "So Many Agent Frameworks‚Ä¶\n\nSource: https://e2b.dev",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-agents-sdk",
    "href": "src/00/ai_agents.html#openai-agents-sdk",
    "title": "Introducing AI Agents",
    "section": "OpenAI Agents SDK",
    "text": "OpenAI Agents SDK\n\nUsed for our demo\nhttps://openai.github.io/openai-agents-python/\nPython and JavaScript/TypeScript\nMIT License\nWorks with models that support OpenAI‚Äôs Responses API",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#langgraph",
    "href": "src/00/ai_agents.html#langgraph",
    "title": "Introducing AI Agents",
    "section": "LangGraph",
    "text": "LangGraph\n\nhttps://langchain-ai.github.io/langgraph/\nPython only\nMIT License",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#crew.ai",
    "href": "src/00/ai_agents.html#crew.ai",
    "title": "Introducing AI Agents",
    "section": "Crew.ai",
    "text": "Crew.ai\n\nhttps://github.com/crewaiinc/crewai\nPython only\nMIT License",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#microsoft",
    "href": "src/00/ai_agents.html#microsoft",
    "title": "Introducing AI Agents",
    "section": "Microsoft",
    "text": "Microsoft\n\nAutoGen\n\nhttps://microsoft.github.io/autogen/stable/\nPython (.NET coming soon)\nMIT License\n\nMicrosoft Semantic Kernel\n\nhttps://github.com/microsoft/semantic-kernel\nPython, .NET, Java\nMIT License",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-should-you-be-considering",
    "href": "src/00/ai_agents.html#what-should-you-be-considering",
    "title": "Introducing AI Agents",
    "section": "What Should You Be Considering?",
    "text": "What Should You Be Considering?\n\nQuestion: If most agents run in the browser, why are many of the agent frameworks written in Python? ü§î",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-should-you-be-considering-1",
    "href": "src/00/ai_agents.html#what-should-you-be-considering-1",
    "title": "Introducing AI Agents",
    "section": "What Should You Be Considering?",
    "text": "What Should You Be Considering?\n\nWhere will your agent run?\n\nServer or client? On the web? In game?\nThis will likely determine the language\nHow will you manage API keys?\n\nSupport\n\nWill this agent framework be around in 3-5 years?\nIs there a cost/hosting component to it?\n\nEasy vs.¬†Simple\n\nIs your choice ‚Äúeasy‚Äù or ‚Äúsimple‚Äù?",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#sidebar-easy-vs.-simple",
    "href": "src/00/ai_agents.html#sidebar-easy-vs.-simple",
    "title": "Introducing AI Agents",
    "section": "Sidebar: Easy vs.¬†Simple",
    "text": "Sidebar: Easy vs.¬†Simple\n\nhttps://www.youtube.com/watch?v=SxdOUGdseq4",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-agents-sdk-python",
    "href": "src/00/ai_agents.html#openai-agents-sdk-python",
    "title": "Introducing AI Agents",
    "section": "OpenAI Agents SDK (Python)",
    "text": "OpenAI Agents SDK (Python)\nCreating and running a new agent\nfrom agents import Agent, Runner\n\nagent = Agent(\n    name=\"DigiPen Campus Assistant\",\n    instructions=\"You are a helpful campus assistant that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)\n\nmessages.append({\"role\": \"user\", \"content\": user_msg})\nresult = Runner.run_streamed(agent, messages)",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-agents-sdk-python-1",
    "href": "src/00/ai_agents.html#openai-agents-sdk-python-1",
    "title": "Introducing AI Agents",
    "section": "OpenAI Agents SDK (Python)",
    "text": "OpenAI Agents SDK (Python)\nSpecifying handoffs\nfrom agents import Agent, Runner\n\nagent = Agent(\n    name=\"DigiPen Campus Assistant\",\n    instructions=\"You are a helpful campus assistant that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)\n\nmessages.append({\"role\": \"user\", \"content\": user_msg})\nresult = Runner.run_streamed(agent, messages)",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#why-multiple-agents",
    "href": "src/00/ai_agents.html#why-multiple-agents",
    "title": "Introducing AI Agents",
    "section": "Why Multiple Agents?",
    "text": "Why Multiple Agents?\n\nContext window limitations\nEach agent can have a different system prompt (instructions)\nEach agent can have a different underlying model\n\nSpecialized models (e.g., a vision encoder)\nOr to blend cost\n\nMakes tool separation cleaner and more accurate",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#example-of-multiple-agents",
    "href": "src/00/ai_agents.html#example-of-multiple-agents",
    "title": "Introducing AI Agents",
    "section": "Example of Multiple Agents",
    "text": "Example of Multiple Agents\n\nCode generation\n\nAgents for ‚Äòarchitect‚Äô, code writer, tester, debugger, etc.\n\nContent generation\n\nAgent to create content, other agents to generate images, translate content, etc.\n\nTravel booking\n\nAgent to book flights, hotels, cars, etc. for packages",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#patterns-for-agents",
    "href": "src/00/ai_agents.html#patterns-for-agents",
    "title": "Introducing AI Agents",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nAs you get deeper into building agents, patterns start to emerge\n\ne.g., router (which is what we used in our demo), orchestrator (using other agents as tools), parallel agents",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#patterns-for-agents-1",
    "href": "src/00/ai_agents.html#patterns-for-agents-1",
    "title": "Introducing AI Agents",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nSource: https://www.anthropic.com/engineering/building-effective-agents",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#why-do-agents-need-tools",
    "href": "src/00/ai_agents.html#why-do-agents-need-tools",
    "title": "Introducing AI Agents",
    "section": "Why Do Agents Need Tools?",
    "text": "Why Do Agents Need Tools?\n\nThe scope of the agents ability is contained within the model\nTools enable the agent to reach out to systems beyond the model\nExamples\n\nRead a file from disk or search the web (built in)\nCalculator (because LLMs aren‚Äôt great at math)\nCode interpreter (running code on the fly)\nWhat potential tools can you think of? ü§î",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-tool-calling",
    "href": "src/00/ai_agents.html#openai-tool-calling",
    "title": "Introducing AI Agents",
    "section": "OpenAI Tool Calling",
    "text": "OpenAI Tool Calling\n\nIntroduced by OpenAI in June 2023\nOriginally called Function Calling\nModels are fine-tuned to return a structured function_call JSON object, specifying which function to call and with what arguments.\nTools are provided as functions\nOption for the LLM to decide when to call the tool (always, never, auto)",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-tool-calling-1",
    "href": "src/00/ai_agents.html#openai-tool-calling-1",
    "title": "Introducing AI Agents",
    "section": "OpenAI Tool Calling",
    "text": "OpenAI Tool Calling\nfrom agents import Agent, function_tool\n\n@function_tool\ndef get_bytes_cafe_menu(date: str) -&gt; any:\n    \"\"\"Returns the menu for the Bytes Cafe for the date provided.\"\"\"\n    return { f\"{date}\": {\n            \"daily byte\": {\n                \"name\": \"Steak Quesadilla\", \"price\": 12, \"description\": \"Flank steak, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n        } }\n\ncafe_agent = Agent(\n    name=\"Cafe Agent\",\n    instructions=\"You help students locate and provide information about the Bytes Cafe.\",\n    tools=[\n        get_bytes_cafe_menu,\n    ])",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-tool-calling-2",
    "href": "src/00/ai_agents.html#openai-tool-calling-2",
    "title": "Introducing AI Agents",
    "section": "OpenAI Tool Calling",
    "text": "OpenAI Tool Calling\nfrom agents import Agent, FileSearchTool\n\nVECTOR_STORE_ID = \"vs_6896d8c959008191981d645850b42313\"\n\nbuilding_agent = Agent(\n    name=\"Building Agent\",\n    instructions=\"You help students locate and provide information about buildings and rooms on campus. Be descriptive when giving locations.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#mcp-model-context-protocol",
    "href": "src/00/ai_agents.html#mcp-model-context-protocol",
    "title": "Introducing AI Agents",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)\n\nOpenAI‚Äôs tool/function calling works if you have local access to the tools\nBut how about external systems?\n\ne.g., Access to calendar, other systems, hardware, etc.\n\nYou could build local tools to call APIs\nMCP: Model Context Protocol\nStandardized tooling interface, released by Anthropic in March 2024\nMore details: https://modelcontextprototcol.io",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#using-an-agent-for-your-project",
    "href": "src/00/ai_agents.html#using-an-agent-for-your-project",
    "title": "Introducing AI Agents",
    "section": "Using an Agent for Your Project",
    "text": "Using an Agent for Your Project\n\nCould your project benefit from agents?\nOne or more agents?\nHow would agents interact with the user?\nWhat might be some of the challenges to overcome?",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#resources-12",
    "href": "src/00/ai_agents.html#resources-12",
    "title": "Introducing AI Agents",
    "section": "Resources (1/2)",
    "text": "Resources (1/2)\n\nThis slide deck, resources, links, everything:\n\nhttps://simonguest.github.io/CSP-400",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#resources-22",
    "href": "src/00/ai_agents.html#resources-22",
    "title": "Introducing AI Agents",
    "section": "Resources (2/2)",
    "text": "Resources (2/2)\n\nCampus agent code:\n\nhttps://github.com/simonguest/CSP-400/tree/main/demos/00/campus-agent\nClone the repo, get it running locally\nModify by adding a new agent to it\nCreate your own use case/agent in the Gradio interface",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#recap",
    "href": "src/01/exploring_genai.html#recap",
    "title": "Exploring Generative AI Models",
    "section": "Recap",
    "text": "Recap\n\nLast Week‚Äôs Lecture\n\nIntroduced AI Agents, their uses, how to create\nAbout 50% had used some kind of API\nOne or two beyond this\n\nThis Week\n\nExplore text and image-based models\nModel evolution, API access, running locally\nDemos!",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#a-brief-history-of-transformer-models-1",
    "href": "src/01/exploring_genai.html#a-brief-history-of-transformer-models-1",
    "title": "Exploring Generative AI Models",
    "section": "A Brief History of Transformer Models",
    "text": "A Brief History of Transformer Models\n\n\n\n\n\ntimeline\n    June 2017 : Google researchers publish \"Attention is all you need\" paper\n              : Introduces self-attention mechanism and transformer architecture\n              : Eliminates the need for recurrent neural networks in sequence processing\n    \n    June 2018 : OpenAI releases GPT-1\n              : 117M parameters\n              : Demonstrates pre-training on large text corpora followed by fine-tuning works effectively\n    \n    Feb 2019 : OpenAI releases GPT-2\n             : 1.5B parameters\n             : Initially withheld full model due to concerns about misuse\n             : Demonstrates impressive text generation capabilities with minimal fine-tuning\n    \n    May 2020 : OpenAI releases GPT-3\n             : 175B parameters\n             : Demonstrates strong few-shot learning capabilities\n             : Marks a significant leap in model capabilities and scale\n    \n    June 2020 : GPT-3 available through OpenAI API\n              : Still a completion model, not instruction-tuned",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#completion-vs.-instruction-tuned",
    "href": "src/01/exploring_genai.html#completion-vs.-instruction-tuned",
    "title": "Exploring Generative AI Models",
    "section": "Completion vs.¬†Instruction-Tuned",
    "text": "Completion vs.¬†Instruction-Tuned\n\nCompletion Model just predicts the next token\n\nInput prompt - Mary had a little\nMax total tokens - 50\nTemperature - 0 - 1.0\ntop_k - consider only the top k tokens in the response\ntop_p - Nucleus sampling (probability cut off - 0 and 1.0)\n\nOutput\n\nMary had a little lamb, its fleece was white as snow... (up to max tokens)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#completion-vs.-instruction-tuned-1",
    "href": "src/01/exploring_genai.html#completion-vs.-instruction-tuned-1",
    "title": "Exploring Generative AI Models",
    "section": "Completion vs.¬†Instruction-Tuned",
    "text": "Completion vs.¬†Instruction-Tuned\n\nYou can‚Äôt really converse with it\nWhat is the capital of France? (max tokens = 50)\nWhat is the capital of France? Paris. What is the capital of Spain? Madrid. What is the capital of",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#introducing-google-colab",
    "href": "src/01/exploring_genai.html#introducing-google-colab",
    "title": "Exploring Generative AI Models",
    "section": "Introducing Google Colab",
    "text": "Introducing Google Colab\n\nSource: https://colab.research.google.com/signup",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#instruction-tuned-models",
    "href": "src/01/exploring_genai.html#instruction-tuned-models",
    "title": "Exploring Generative AI Models",
    "section": "Instruction-Tuned Models",
    "text": "Instruction-Tuned Models\n\nSupervised Fine-Tuning\n\nLarge datasets of questions/answers, tasks/completions, demonstrating helpful assistant behavior\n\nRLHF (Reinforcement Learning from Human Feedback)\n\nHuman raters rank different model responses, training a reward model\n\nChat Templates\n\nStructured formats to distinguish speakers in a dialog: Typically system, user, and assistant",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#a-brief-history-of-transformer-models-2",
    "href": "src/01/exploring_genai.html#a-brief-history-of-transformer-models-2",
    "title": "Exploring Generative AI Models",
    "section": "A Brief History of Transformer Models",
    "text": "A Brief History of Transformer Models\n\n\n\n\n\ntimeline\n    2021 : InstructGPT Development\n          : Built on GPT-3 with RLHF fine-tuning\n          : Trained to follow instructions and understand user intent\n          : Key innovation enabling ChatGPT\n    \n    Jan 2021 : Anthropic Founded\n             : Founded by Dario & Daniela Amodei with ~7 senior OpenAI employees\n            : Dario led GPT-2/3 development and co-invented RLHF\n\n    Nov 2022 : ChatGPT Launch\n                  : Built on GPT-3.5 using RLHF\n                  : 1M+ users in 5 days\n                  : Sparked widespread interest in generative AI\n\n    Feb 2023 : Llama 1 Released\n                  : Meta's LLaMA (7B, 13B, 33B, 65B parameters)\n                  : 13B model exceeded GPT-3 (175B) on most benchmarks\n                  : Limited researcher access\n                  : Text completion only (Alpaca fine-tune added instructions)\n\n    Jul 2023 : Llama 2 Released\n              : Available in 7B, 13B, 70B sizes\n              : Trained on 40% more data than Llama 1\n              : First open-weights Llama for commercial use",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#closed-vs.-open-models",
    "href": "src/01/exploring_genai.html#closed-vs.-open-models",
    "title": "Exploring Generative AI Models",
    "section": "Closed vs.¬†Open Models",
    "text": "Closed vs.¬†Open Models\n\nClosed Source: Hosted models; no ability to inspect the weights of the models. Accessed via an API (or UI).\n\nExamples: OpenAI GPT-5, Claude Sonnet 4.5\n\nOpen Weight: Model files with pretrained weights, but no training data. Host on your own hardware.\n\nExamples: Meta‚Äôs Llama (and derivatives), Mistral\n\nOpen Source Models: Models with access to the training data set. Create from scratch.\n\nExamples: OLMo from AI2",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#accessing-models-via-apis",
    "href": "src/01/exploring_genai.html#accessing-models-via-apis",
    "title": "Exploring Generative AI Models",
    "section": "Accessing Models via APIs",
    "text": "Accessing Models via APIs\n\nHTTP-based APIs\n\nClient makes HTTP API calls to invoke/access the model\n(Normally use an SDK to wrap the HTTP API calls)\nClient passes Authorization token as part of the call\nDefault way of accesing OpenAI, Claude, other large, closed-source models",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#accessing-models-via-apis-1",
    "href": "src/01/exploring_genai.html#accessing-models-via-apis-1",
    "title": "Exploring Generative AI Models",
    "section": "Accessing Models via APIs",
    "text": "Accessing Models via APIs\n\nCoLab DEMO: Using the OpenAI SDK to access ChatGPT, explore the OpenAI chat format",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#accessing-models-via-apis-2",
    "href": "src/01/exploring_genai.html#accessing-models-via-apis-2",
    "title": "Exploring Generative AI Models",
    "section": "Accessing Models via APIs",
    "text": "Accessing Models via APIs",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#openais-chat-completions-api",
    "href": "src/01/exploring_genai.html#openais-chat-completions-api",
    "title": "Exploring Generative AI Models",
    "section": "OpenAI‚Äôs Chat Completions API",
    "text": "OpenAI‚Äôs Chat Completions API\n\nDebuted in March 2023, together with the ChatGPT API\nStructure\n\nMessages array (system, assistant, user)\nStreaming support (using SSE)\nSimple parameters\nFunction calling (added mid-2023)\n\nWidespread Adoption\n\nLangchain, other SDKs\nAnthropic, Azure, TogetherAI\nLocal hosting: vLLM, LM Studio",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#introducing-openrouter",
    "href": "src/01/exploring_genai.html#introducing-openrouter",
    "title": "Exploring Generative AI Models",
    "section": "Introducing OpenRouter",
    "text": "Introducing OpenRouter\nHow about creating a client that consumes different models from multiple providers?\n\nIntroducing OpenRouter (https://openrouter.ai)\n\nA unified API to hundreds of AI models through a single endpoint\nOpenAI, Claude, Gemini, Grok, Nova, Llama, DeepSeek, Qwen and many others.\nOpenAI Chat Completions compatible",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-openrouter",
    "href": "src/01/exploring_genai.html#demo-openrouter",
    "title": "Exploring Generative AI Models",
    "section": "Demo: OpenRouter",
    "text": "Demo: OpenRouter\nOpenRouter.ipynb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#downloading-and-running-models",
    "href": "src/01/exploring_genai.html#downloading-and-running-models",
    "title": "Exploring Generative AI Models",
    "section": "Downloading and Running Models",
    "text": "Downloading and Running Models\n\nSo far, we‚Äôve called hosted models via APIs\nHow about downloading and running models on your own hardware?\n\n(Obviously they need to be open-weight models)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#downloading-and-running-models-1",
    "href": "src/01/exploring_genai.html#downloading-and-running-models-1",
    "title": "Exploring Generative AI Models",
    "section": "Downloading and Running Models",
    "text": "Downloading and Running Models\nWhy would you want to do this?\n\nOffline access to models (no Internet required)\nPotential cost savings (if many API calls and already own hardware)\n\ne.g., running a small model embedded within a game\n\nWant to fine-tune your own model and have the hardware to do it\nDon‚Äôt want others to see what types of conversations you are having :)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#introducing-hugging-face",
    "href": "src/01/exploring_genai.html#introducing-hugging-face",
    "title": "Exploring Generative AI Models",
    "section": "Introducing Hugging Face",
    "text": "Introducing Hugging Face\n\nSource: https://huggingface.co",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#what-is-hugging-face",
    "href": "src/01/exploring_genai.html#what-is-hugging-face",
    "title": "Exploring Generative AI Models",
    "section": "What is Hugging Face?",
    "text": "What is Hugging Face?\nIt is to AI models what GitHub is to source code\n\nExplore, download models to run on local hardware\nUpload and share your own trained/fine-tuned models and datasets\nCreate ‚ÄúSpaces‚Äù - web-based apps for accessing models",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-exploring-a-model-on-hugging-face",
    "href": "src/01/exploring_genai.html#demo-exploring-a-model-on-hugging-face",
    "title": "Exploring Generative AI Models",
    "section": "Demo: Exploring a Model on Hugging Face",
    "text": "Demo: Exploring a Model on Hugging Face\nGoogle‚Äôs gemma-3-1b-it on Hugging Face",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#hugging-face-transformers",
    "href": "src/01/exploring_genai.html#hugging-face-transformers",
    "title": "Exploring Generative AI Models",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\n\nSource: https://huggingface.co/docs/transformers",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#hugging-face-transformers-1",
    "href": "src/01/exploring_genai.html#hugging-face-transformers-1",
    "title": "Exploring Generative AI Models",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\nWhat is the Hugging Face Transformers Library?\n\nOpen-source Python library to provide easy access to using various types of pre-trained transformer models\nBrings together all of the different formats under one interface.\n\nDifferent models, vendors, types, chat templates\nDifferent implementations: PyTorch, TensorFlow, JAX\n\nA few lines of code to download and run the model",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-using-transformers-lib-to-download-and-use-gemma-3-1b",
    "href": "src/01/exploring_genai.html#demo-using-transformers-lib-to-download-and-use-gemma-3-1b",
    "title": "Exploring Generative AI Models",
    "section": "Demo: Using Transformers Lib to download and use Gemma 3 1B",
    "text": "Demo: Using Transformers Lib to download and use Gemma 3 1B\ngemma-3-1b-it via transformers.ipynb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#vram-envy",
    "href": "src/01/exploring_genai.html#vram-envy",
    "title": "Exploring Generative AI Models",
    "section": "VRAM Envy",
    "text": "VRAM Envy\nOne challenge of running moels on your own hardware is VRAM availability!\n\nRoughly speaking, the size of the model will determine how much VRAM you need\nGemma 3 models\n\ngemma-3-1b-it = 2Gb\ngemma-3-4b-it = 8.6Gb\ngemma-3-12b-it = 23.37Gb\n\nColab Tiers\n\nColab Free T4 = 16Gb VRAM (15Gb usable)\nColab Pro V100 = 16Gb VRAM\nColab Pro A100 = 40Gb VRAM\n\nYour Hardware\n\nProbably 8Gb VRAM :)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#vram-envy-1",
    "href": "src/01/exploring_genai.html#vram-envy-1",
    "title": "Exploring Generative AI Models",
    "section": "VRAM Envy",
    "text": "VRAM Envy\nYou can select smaller models, but they are less accurate / more prone to hallucination.\n\nHow do we fix this?\n\nQuantization",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#quantization",
    "href": "src/01/exploring_genai.html#quantization",
    "title": "Exploring Generative AI Models",
    "section": "Quantization",
    "text": "Quantization\nProcess of reducing the precision of a model‚Äôs weights and activations. For example, 16-bit numbers to 4-bit.\n\nParameter count matters more than precision\n\nA 70B parameter model at 4-bit often beats a 13B model at b16\nThe models knowledge remains largely intact\nOften the extra precision doesn‚Äôt meaningfully improve outputs",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#quantization-formats",
    "href": "src/01/exploring_genai.html#quantization-formats",
    "title": "Exploring Generative AI Models",
    "section": "Quantization Formats",
    "text": "Quantization Formats\nThe llama.cpp project (implementing LLMs in pure C/C++) has driven advancements in quantization\n\nGGUF (GPT-Generated Unifed Format)\n\nSingle file architecture\nModel format supporting multiple quantization levels (2-bit through 8-bit)\n\nMLX (Apple‚Äôs ML framework and format for Apple Silicon)\n\nDebuted in late 2023\nSupports 4 and 8 bit quantization schemes\n\nTools built upon llama.cpp\n\nOllama, LM Studio, koboldcpp",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-running-gemma-3-27b-gguf-on-my-laptop",
    "href": "src/01/exploring_genai.html#demo-running-gemma-3-27b-gguf-on-my-laptop",
    "title": "Exploring Generative AI Models",
    "section": "Demo: Running Gemma 3 27B GGUF on my laptop",
    "text": "Demo: Running Gemma 3 27B GGUF on my laptop\n\nLMStudio: https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-gguf\n55Gb at b16, 16Gb quantized",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#going-beyond-text-1",
    "href": "src/01/exploring_genai.html#going-beyond-text-1",
    "title": "Exploring Generative AI Models",
    "section": "Going Beyond Text",
    "text": "Going Beyond Text\nUp to now, we‚Äôve concentrated on NLP (text generation) models. Other categories emerging for generative models:\n\nImage Models\n\nGeneration\nVision Transformers\n\nMultimodal Models\nAudio Models\nRL Models\n\nFor the rest of this presentation, introduction into image models",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#images-models---generation",
    "href": "src/01/exploring_genai.html#images-models---generation",
    "title": "Exploring Generative AI Models",
    "section": "Images Models - Generation",
    "text": "Images Models - Generation\nText-to-Image: ‚ÄúA photograph of an astronaut riding a horse.‚Äù\n\nBased on a concept called a diffuser\nTwo stage process, inspired by thermodynamics\nTraining\n\nDuring training, random noise is adding to images in steps\nModel learns to predict what noise what added (forward diffusion process)\n\nInference\n\nTo generate new images, the process runs in reverse\nStart with pure random noise (known as a seed)\nModel estimates what noise should be removed to create a realistic image\nUsing the text prompt, the model steers the process towards images that match the description\n\n\nTBD: sources (don‚Äôt forget sources throughout)\n2015 - Deep Unsupervised Learning using Nonequilibrium Thermodynamics (Sohl-Dickstein et al.) 2019 - Generative Modeling by Estimating Gradients of the Data Distribution (Song & Ermon) 2020 - Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., UC Berkeley)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-diffusion-models-in-2022",
    "href": "src/01/exploring_genai.html#image-diffusion-models-in-2022",
    "title": "Exploring Generative AI Models",
    "section": "Image Diffusion Models in 2022",
    "text": "Image Diffusion Models in 2022\n\n\n\n\n\ntimeline\n  August 2022 : Stable Diffusion v1.4\n              : First open-source high-quality model\n  September 2022 : Stable Diffusion v1.5\n                  : Refined version\n  October 2022 : eDiff-I (NVIDIA)\n                : Ensemble approach\n  November 2022 : Stable Diffusion v2.0/2.1\n                : Higher resolution (768x768)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-diffusion-models-in-2023",
    "href": "src/01/exploring_genai.html#image-diffusion-models-in-2023",
    "title": "Exploring Generative AI Models",
    "section": "Image Diffusion Models in 2023",
    "text": "Image Diffusion Models in 2023\n\n\n\n\n\ntimeline\n  March 2023 : Midjourney v5\n              : Exceptional artistic quality\n  April 2023 : ControlNet\n        : Precise spatial control\n        : AnimateDiff - Video generation\n  July 2023 : SDXL (Stable Diffusion XL)\n            : 1024x1024 native resolution\n  August 2023 : SDXL Turbo\n              : Real-time capable generation",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-diffusion-models-in-2024",
    "href": "src/01/exploring_genai.html#image-diffusion-models-in-2024",
    "title": "Exploring Generative AI Models",
    "section": "Image Diffusion Models in 2024",
    "text": "Image Diffusion Models in 2024\n\n\n\n\n\ntimeline\n  February 2024 : Sora (OpenAI)\n                : Text-to-video up to 60 seconds\n                : Stable Diffusion 3\n                : Improved text understanding\n  June 2024 : Stable Diffusion 3.5\n            : Multiple model sizes\n  2024 : FLUX.1 (Black Forest Labs)\n        : State-of-the-art open model\n        : Imagen 3 (Google DeepMind)\n        : Photorealistic quality",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-models---generation",
    "href": "src/01/exploring_genai.html#image-models---generation",
    "title": "Exploring Generative AI Models",
    "section": "Image Models - Generation",
    "text": "Image Models - Generation\nTBD: Insert image from Diffusion Process notebook",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-models---generation-1",
    "href": "src/01/exploring_genai.html#image-models---generation-1",
    "title": "Exploring Generative AI Models",
    "section": "Image Models - Generation",
    "text": "Image Models - Generation\nImage-to-Image: ‚ÄúMake this image different‚Äù\n\nOriginally solved by GAN approaches, but evolved into extension of the diffusion concept\n\nAdd noise to the original image (partial denoising)\nRegenerate it with modifications based on the prompt\nThe original image heavily influences the output structure",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-models---image-to-image-example",
    "href": "src/01/exploring_genai.html#image-models---image-to-image-example",
    "title": "Exploring Generative AI Models",
    "section": "Image Models - Image-to-Image example",
    "text": "Image Models - Image-to-Image example\nStable Diffusion with img2img (2022) - Popularized diffusion-based image-to-image for the masses. It works by encoding the input image to latent space, adding noise, then denoising with text conditioning. The ‚Äústrength‚Äù parameter controls how much the original image influences the result.\nTBD: Link to Colab Demo",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#controlnet",
    "href": "src/01/exploring_genai.html#controlnet",
    "title": "Exploring Generative AI Models",
    "section": "ControlNet",
    "text": "ControlNet\nText-to-Image and Image-to-Image are interesting, but you don‚Äôt really have that much control.\nThere are a lot of fine-tuning techniques, but it‚Äôs expensive and risk degrading quality.\nControlNet was developed by Lvmin Zhang and Maneesh Agrawala at Stanford University, published in February 2023 in the paper ‚ÄúAdding Conditional Control to Text-to-Image Diffusion Models.‚Äù\nControlNet represented a paradigm shift from ‚Äúdescribe what you want‚Äù to ‚Äúshow the structure you want.‚Äù",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#how-controlnet-works",
    "href": "src/01/exploring_genai.html#how-controlnet-works",
    "title": "Exploring Generative AI Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\nStable Diffusion‚Äôs U-Net has an encoder and decoder\nCreate a trainable copy of the encoder blocks\nTrain the copy of the encoder alongside the frozen SD model\n\nDuring training: use paired data (e.g., pose skeleton ‚Üí original image)\nDuring inference: both encoders run together - original processes noisy latent, copy processes control input\nFeatures from both are combined via zero convolutions\n\nKey: The weights in the original SD model doesn‚Äôt change\nAnalogous to ControlNet being a ‚ÄúPlug in‚Äù model",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#controlnet-1",
    "href": "src/01/exploring_genai.html#controlnet-1",
    "title": "Exploring Generative AI Models",
    "section": "ControlNet",
    "text": "ControlNet\nDemo: ControlNet with human pose detection",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#controlnet-2",
    "href": "src/01/exploring_genai.html#controlnet-2",
    "title": "Exploring Generative AI Models",
    "section": "ControlNet",
    "text": "ControlNet\nExmaples of conditioning types\n\nCanny edges - line drawings and edge detection\nDepth maps - 3D structure information\nNormal maps - surface orientation\nHuman pose (OpenPose) - skeleton/keypoint detection\nSemantic segmentation - labeled regions\nScribbles - rough user drawings\nHED boundaries - soft edge detection",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#controlnet-based-methods",
    "href": "src/01/exploring_genai.html#controlnet-based-methods",
    "title": "Exploring Generative AI Models",
    "section": "ControlNet-Based Methods",
    "text": "ControlNet-Based Methods\n\nAdvanced Editing Techniques\n\nScribble to image / QR code to image\nInpainting (better at large variations vs.¬†other inpainting techniques).\nImage Alteration - e.g., Weather and Environmental Enhancement\n\nStructural and Geometric Transformation\n\nPose Estimation\nNormal Map Generation\nSematic Segmentation\n\nNeural Style Transfer\n\nGhibli images, anyone?",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#tools-for-exploring-image-models",
    "href": "src/01/exploring_genai.html#tools-for-exploring-image-models",
    "title": "Exploring Generative AI Models",
    "section": "Tools for Exploring Image Models",
    "text": "Tools for Exploring Image Models\nThere are lots of image models that we didn‚Äôt cover.\n\nReplicate\nComfyUI\nWeavyUI",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-models---vision-transformers",
    "href": "src/01/exploring_genai.html#image-models---vision-transformers",
    "title": "Exploring Generative AI Models",
    "section": "Image Models - Vision Transformers",
    "text": "Image Models - Vision Transformers\nHow about the other way?\n\nClassic models are typically supervised classification tasks\n\nIs this a cat?\nEnd up taking lots of pictures of cats, human annotation, etc.\n\nIntroducing the Vision Transformer (ViT)\n\n‚ÄúAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale‚Äù\nMore on this in the next lecture!",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#resources",
    "href": "src/01/exploring_genai.html#resources",
    "title": "Exploring Generative AI Models",
    "section": "Resources",
    "text": "Resources\nTBD",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#qa",
    "href": "src/01/exploring_genai.html#qa",
    "title": "Exploring Generative AI Models",
    "section": "Q&A",
    "text": "Q&A",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/networking/http3.html",
    "href": "src/networking/http3.html",
    "title": "HTTP/3",
    "section": "",
    "text": "HTTP/3 is the latest major revision of the Hypertext Transfer Protocol, designed to make web communication faster, more reliable, and better suited for modern network conditions. Unlike HTTP/1.1 and HTTP/2, which are built on top of TCP, HTTP/3 runs over¬†QUIC, a transport protocol based on UDP. By leveraging QUIC, HTTP/3 benefits from features such as built-in encryption (TLS 1.3), multiplexed streams without head-of-line blocking, reduced connection setup time, and graceful handling of network changes (e.g., switching from WiFi to cellular).\nHTTP/3 is increasingly used by browsers, CDNs, and backend services to improve page load times, streaming performance, and resilience to packet loss in high-latency networks.\n\n\n\n\n\nflowchart LR\n    subgraph Client[\"Client (e.g., Browser, Mobile App)\"]\n        C1[HTTP/3 Stack]\n        C2[Application Logic]\n        C2 --&gt; C1\n    end\n\n    subgraph Server[\"Server\"]\n        S1[HTTP/3-capable Web Server / CDN]\n        S2[Application Logic / Handlers]\n        S3[Database / External APIs]\n        S2 --&gt; S3\n        S1 --&gt; S2\n    end\n\n    C1 &lt;--&gt; |\"QUIC (UDP-based) Transport\"| S1\n\n    %% Styling\n    style Client fill:#f0f9ff,stroke:#0369a1,stroke-width:2px,rounded-corners:5px\n    style Server fill:#fef9c3,stroke:#ca8a04,stroke-width:2px,rounded-corners:5px\n\n\n\n\n\n\n\n\n\nThe most widely used Python library for HTTP/3 and QUIC is¬†aioquic.\n\nIt‚Äôs an implementation of QUIC and HTTP/3 built on top of¬†asyncio.\nSupports client and server roles, TLS 1.3, and custom protocol experimentation.\nUseful for both low-level QUIC experiments and HTTP/3 integration with web servers.\n\nOther frameworks are beginning to add¬†HTTP/3 support¬†(e.g., Hypercorn or uvicorn with custom workers), but¬†aioquic¬†remains the go-to library today.\n\n\n\n\n\nEcosystem Maturity¬†‚Äì HTTP/3 adoption is growing, but not all clients, servers, proxies, or CDNs support it yet.\nUDP Dependency¬†‚Äì Some networks (corporate firewalls, older routers) may block or throttle UDP traffic, limiting HTTP/3‚Äôs performance benefits.\nResource Usage¬†‚Äì Running on top of UDP requires more aggressive congestion control and buffer handling, which may make server-side scaling more complex.\nDebugging & Monitoring¬†‚Äì Tools for packet inspection and debugging are less mature than for TCP/HTTP/2.\nFallback Complexity¬†‚Äì Many servers and browsers negotiate between HTTP/1.1, HTTP/2, and HTTP/3, which adds operational complexity.\nNot Always Faster¬†‚Äì For very stable, low-latency connections, HTTP/2 may perform competitively; HTTP/3 shines in lossy or high-latency links.",
    "crumbs": [
      "Networking Protocols",
      "HTTP/3"
    ]
  },
  {
    "objectID": "src/networking/http3.html#python-sdk",
    "href": "src/networking/http3.html#python-sdk",
    "title": "HTTP/3",
    "section": "",
    "text": "The most widely used Python library for HTTP/3 and QUIC is¬†aioquic.\n\nIt‚Äôs an implementation of QUIC and HTTP/3 built on top of¬†asyncio.\nSupports client and server roles, TLS 1.3, and custom protocol experimentation.\nUseful for both low-level QUIC experiments and HTTP/3 integration with web servers.\n\nOther frameworks are beginning to add¬†HTTP/3 support¬†(e.g., Hypercorn or uvicorn with custom workers), but¬†aioquic¬†remains the go-to library today.",
    "crumbs": [
      "Networking Protocols",
      "HTTP/3"
    ]
  },
  {
    "objectID": "src/networking/http3.html#limitations",
    "href": "src/networking/http3.html#limitations",
    "title": "HTTP/3",
    "section": "",
    "text": "Ecosystem Maturity¬†‚Äì HTTP/3 adoption is growing, but not all clients, servers, proxies, or CDNs support it yet.\nUDP Dependency¬†‚Äì Some networks (corporate firewalls, older routers) may block or throttle UDP traffic, limiting HTTP/3‚Äôs performance benefits.\nResource Usage¬†‚Äì Running on top of UDP requires more aggressive congestion control and buffer handling, which may make server-side scaling more complex.\nDebugging & Monitoring¬†‚Äì Tools for packet inspection and debugging are less mature than for TCP/HTTP/2.\nFallback Complexity¬†‚Äì Many servers and browsers negotiate between HTTP/1.1, HTTP/2, and HTTP/3, which adds operational complexity.\nNot Always Faster¬†‚Äì For very stable, low-latency connections, HTTP/2 may perform competitively; HTTP/3 shines in lossy or high-latency links.",
    "crumbs": [
      "Networking Protocols",
      "HTTP/3"
    ]
  },
  {
    "objectID": "src/networking/webrtc.html",
    "href": "src/networking/webrtc.html",
    "title": "WebRTC",
    "section": "",
    "text": "WebRTC (Web Real-Time Communication) is a set of APIs and protocols that enables direct peer-to-peer communication between clients (such as web browsers or mobile apps) without routing all traffic through a central server. It supports real-time transfer of audio, video, and arbitrary data, making it ideal for video conferencing, voice calls, live streaming, and real-time file sharing. Unlike WebSockets, which always rely on a continuous connection with a server, WebRTC establishes a direct connection between peers, often requiring signaling servers and STUN/TURN servers to help with connection setup and NAT/firewall traversal.\n\n\n\n\n\nflowchart LR\n    subgraph ClientA[\"Client A (e.g., Browser, Mobile App)\"]\n        A1[WebRTC API]\n        A2[App Logic]\n        A2 --&gt; A1\n    end\n\n    subgraph ClientB[\"Client B (e.g., Browser, Mobile App)\"]\n        B1[WebRTC API]\n        B2[App Logic]\n        B2 --&gt; B1\n    end\n\n    subgraph Signaling[\"Signaling Server\"]\n        S1[Exchange Session Descriptions & ICE Candidates]\n    end\n\n    subgraph STUN_TURN[\"STUN / TURN Servers\"]\n        N1[Helps with NAT Traversal / Relay Media if Needed]\n    end\n\n    %% Corrected edges (no parentheses or slashes in text)\n    A1 &lt;--&gt; |\"Signaling over WebSocket or HTTP\"| S1\n    B1 &lt;--&gt; |\"Signaling over WebSocket or HTTP\"| S1\n    A1 &lt;--&gt; |\"Peer-to-Peer Media & Data\"| B1\n    A1 &lt;--&gt; N1\n    B1 &lt;--&gt; N1\n\n    %% Styling\n    style ClientA fill:#f0f9ff,stroke:#0369a1,stroke-width:2px,rounded-corners:5px\n    style ClientB fill:#ede9fe,stroke:#6d28d9,stroke-width:2px,rounded-corners:5px\n    style Signaling fill:#fef9c3,stroke:#ca8a04,stroke-width:2px,rounded-corners:5px\n    style STUN_TURN fill:#dcfce7,stroke:#15803d,stroke-width:2px,rounded-corners:5px",
    "crumbs": [
      "Networking Protocols",
      "WebRTC"
    ]
  },
  {
    "objectID": "src/networking/webrtc.html#limitations",
    "href": "src/networking/webrtc.html#limitations",
    "title": "WebRTC",
    "section": "Limitations",
    "text": "Limitations\n\nSignaling Dependency¬†‚Äì WebRTC itself does not specify how peers discover and exchange connection information; a separate signaling mechanism (e.g., WebSockets or HTTP) is always required.\nNAT and Firewall Traversal¬†‚Äì Direct peer-to-peer connections can fail in restrictive network environments, making STUN/TURN servers necessary, which adds complexity and cost.\nScalability¬†‚Äì Peer-to-peer topologies work well for small groups, but scaling to large calls (e.g., many participants in a video conference) typically requires media servers (SFU/MCU).\nImplementation Complexity¬†‚Äì WebRTC supports multiple codecs, transport mechanisms, and NAT traversal strategies, which can be complex to configure correctly.\nResource Intensive¬†‚Äì Real-time audio and video encoding/decoding can be CPU and bandwidth heavy, especially on lower-powered devices.\nLimited Server Control¬†‚Äì Since traffic often flows directly between peers, servers have less visibility and control compared to client-server communication models like WebSockets.",
    "crumbs": [
      "Networking Protocols",
      "WebRTC"
    ]
  }
]