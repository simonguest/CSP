[
  {
    "objectID": "src/networking/websockets.html",
    "href": "src/networking/websockets.html",
    "title": "WebSockets",
    "section": "",
    "text": "WebSockets is a communication protocol that enables full-duplex, bidirectional interaction between a client (such as a web browser) and a server over a single, long-lived connection. Unlike traditional HTTP, which follows a request-response model, WebSockets establish a persistent channel that allows both parties to send and receive data in real time without repeatedly opening new connections. This makes it highly efficient for scenarios where low latency and continuous updates are critical, such as chat applications, live notifications, multiplayer games, and real-time data streaming.\n\n\n\n\n\nflowchart LR\n    subgraph Client[\"Client (e.g., Browser, Mobile App)\"]\n        C1[WebSocket Client Library]\n        C2[Application Logic]\n        C2 --&gt; C1\n    end\n\n    subgraph Server[\"Server\"]\n        S1[WebSocket Server Endpoint]\n        S2[Application Logic / Handlers]\n        S3[Database / External APIs]\n        S2 --&gt; S3\n        S1 --&gt; S2\n    end\n\n    C1 &lt;--&gt; |Persistent WebSocket Connection| S1\n\n    %% Styling for readability\n    style Client fill:#f0f9ff,stroke:#0369a1,stroke-width:2px,rounded-corners:5px\n    style Server fill:#fef9c3,stroke:#ca8a04,stroke-width:2px,rounded-corners:5px\n\n\n\n\n\n\n\n\nThe most popular WebSockets library for Python is¬†websockets, a lightweight, easy-to-use, and widely adopted library that provides both client and server implementations conforming to the WebSocket protocol. It is built on top of Python‚Äôs¬†asyncio, making it well-suited for asynchronous, real-time applications, and is often recommended as the go-to choice in the Python ecosystem for WebSocket development.\n\n\n\n\nScalability Challenges¬†‚Äì Since WebSockets keep long-lived persistent connections, managing thousands or millions of concurrent clients requires careful scaling strategies (load balancing, connection sharding, etc.).\nServer Resource Usage¬†‚Äì Each connection consumes memory and file descriptors on the server, so inefficient handling can lead to resource exhaustion.\nComplexity Compared to HTTP¬†‚Äì WebSockets don‚Äôt have the same mature ecosystem of caching, load balancing, or monitoring that HTTP-based systems do, so debugging and managing them can be harder.\nFirewall and Proxy Compatibility¬†‚Äì Some firewalls, proxies, and enterprise networks may block or disrupt WebSocket traffic since it breaks the traditional request/response model.\nStatelessness Loss¬†‚Äì Unlike HTTP, WebSockets create stateful, continuous connections, which can complicate server restarts, failover, or horizontal scaling.\nLimited Built-in Features¬†‚Äì The protocol is low-level; things like authentication, reconnection logic, and message acknowledgment need to be implemented by the developer (or use a higher-level abstraction like¬†Socket.IO).",
    "crumbs": [
      "Networking Protocols",
      "WebSockets"
    ]
  },
  {
    "objectID": "src/networking/websockets.html#python-sdk",
    "href": "src/networking/websockets.html#python-sdk",
    "title": "WebSockets",
    "section": "",
    "text": "The most popular WebSockets library for Python is¬†websockets, a lightweight, easy-to-use, and widely adopted library that provides both client and server implementations conforming to the WebSocket protocol. It is built on top of Python‚Äôs¬†asyncio, making it well-suited for asynchronous, real-time applications, and is often recommended as the go-to choice in the Python ecosystem for WebSocket development.",
    "crumbs": [
      "Networking Protocols",
      "WebSockets"
    ]
  },
  {
    "objectID": "src/networking/websockets.html#limitations",
    "href": "src/networking/websockets.html#limitations",
    "title": "WebSockets",
    "section": "",
    "text": "Scalability Challenges¬†‚Äì Since WebSockets keep long-lived persistent connections, managing thousands or millions of concurrent clients requires careful scaling strategies (load balancing, connection sharding, etc.).\nServer Resource Usage¬†‚Äì Each connection consumes memory and file descriptors on the server, so inefficient handling can lead to resource exhaustion.\nComplexity Compared to HTTP¬†‚Äì WebSockets don‚Äôt have the same mature ecosystem of caching, load balancing, or monitoring that HTTP-based systems do, so debugging and managing them can be harder.\nFirewall and Proxy Compatibility¬†‚Äì Some firewalls, proxies, and enterprise networks may block or disrupt WebSocket traffic since it breaks the traditional request/response model.\nStatelessness Loss¬†‚Äì Unlike HTTP, WebSockets create stateful, continuous connections, which can complicate server restarts, failover, or horizontal scaling.\nLimited Built-in Features¬†‚Äì The protocol is low-level; things like authentication, reconnection logic, and message acknowledgment need to be implemented by the developer (or use a higher-level abstraction like¬†Socket.IO).",
    "crumbs": [
      "Networking Protocols",
      "WebSockets"
    ]
  },
  {
    "objectID": "src/networking/webrtc-sfu.html",
    "href": "src/networking/webrtc-sfu.html",
    "title": "WebRTC using an SFU (Selective Forwarding Unit)",
    "section": "",
    "text": "This shows how you scale beyond the pure peer-to-peer model (where each participant connects to every other participant). SFUs relay streams between peers, reducing load on clients.\n\n\n\n\n\nflowchart LR\n    subgraph Client1[\"Client 1\"]\n        C1A[WebRTC API]\n        C1B[App Logic]\n        C1B --&gt; C1A\n    end\n\n    subgraph Client2[\"Client 2\"]\n        C2A[WebRTC API]\n        C2B[App Logic]\n        C2B --&gt; C2A\n    end\n\n    subgraph Client3[\"Client 3\"]\n        C3A[WebRTC API]\n        C3B[App Logic]\n        C3B --&gt; C3A\n    end\n\n    subgraph SFU[\"SFU (Selective Forwarding Unit)\"]\n        S1[Media Router / Forwarder]\n    end\n\n    subgraph Signaling[\"Signaling Server\"]\n        Sig[Coordinate Session Setup]\n    end\n\n    subgraph STUN_TURN[\"STUN / TURN Servers\"]\n        N1[NAT Traversal / Relay if Needed]\n    end\n\n    %% Connections\n    C1A &lt;--&gt; |\"Signaling\"| Sig\n    C2A &lt;--&gt; |\"Signaling\"| Sig\n    C3A &lt;--&gt; |\"Signaling\"| Sig\n\n    C1A &lt;--&gt; |\"Media / Data\"| S1\n    C2A &lt;--&gt; |\"Media / Data\"| S1\n    C3A &lt;--&gt; |\"Media / Data\"| S1\n\n    C1A &lt;--&gt; N1\n    C2A &lt;--&gt; N1\n    C3A &lt;--&gt; N1\n\n    %% Styles\n    style Client1 fill:#f0f9ff,stroke:#0369a1,stroke-width:2px,rounded-corners:5px\n    style Client2 fill:#ede9fe,stroke:#6d28d9,stroke-width:2px,rounded-corners:5px\n    style Client3 fill:#fff7ed,stroke:#c2410c,stroke-width:2px,rounded-corners:5px\n    style SFU fill:#fde68a,stroke:#b45309,stroke-width:2px,rounded-corners:5px\n    style Signaling fill:#fef9c3,stroke:#ca8a04,stroke-width:2px,rounded-corners:5px\n    style STUN_TURN fill:#dcfce7,stroke:#15803d,stroke-width:2px,rounded-corners:5px\n\n\n\n\n\n\n\n\n\nClients (1,2,3‚Ä¶)¬†send their audio/video streams to the¬†SFU.\nThe¬†SFU¬†forwards each stream to the other clients, without mixing them (lightweight compared to MCU mixing).\nSignaling server¬†is still used for session setup (e.g., exchanging SDP & ICE candidates).\nSTUN/TURN servers¬†may be needed for NAT traversal or relaying in restrictive networks.\n\n\n\n\nEach client¬†uploads their media once¬†(instead of N times in pure P2P).\nSFU handles redistribution, keeping CPU/bandwidth low for clients.\nWorks well for¬†group calls, webinars, multi-user streaming.",
    "crumbs": [
      "Networking Protocols",
      "WebRTC using an SFU"
    ]
  },
  {
    "objectID": "src/networking/webrtc-sfu.html#sfu-based-webrtc-architecture-scalability",
    "href": "src/networking/webrtc-sfu.html#sfu-based-webrtc-architecture-scalability",
    "title": "WebRTC using an SFU (Selective Forwarding Unit)",
    "section": "",
    "text": "Clients (1,2,3‚Ä¶)¬†send their audio/video streams to the¬†SFU.\nThe¬†SFU¬†forwards each stream to the other clients, without mixing them (lightweight compared to MCU mixing).\nSignaling server¬†is still used for session setup (e.g., exchanging SDP & ICE candidates).\nSTUN/TURN servers¬†may be needed for NAT traversal or relaying in restrictive networks.\n\n\n\n\nEach client¬†uploads their media once¬†(instead of N times in pure P2P).\nSFU handles redistribution, keeping CPU/bandwidth low for clients.\nWorks well for¬†group calls, webinars, multi-user streaming.",
    "crumbs": [
      "Networking Protocols",
      "WebRTC using an SFU"
    ]
  },
  {
    "objectID": "src/01/resources.html",
    "href": "src/01/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This document contains all the external resources, links, and references mentioned in the ‚ÄúExploring Generative AI Models‚Äù lecture.",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#foundational-papers",
    "href": "src/01/resources.html#foundational-papers",
    "title": "Resources",
    "section": "Foundational Papers",
    "text": "Foundational Papers\n\nAttention Is All You Need (2017) - Vaswani et al.\n\nOriginal transformer paper from Google Research\nhttps://arxiv.org/abs/1706.03762\n\nAdding Conditional Control to Text-to-Image Diffusion Models (2023) - Zhang & Agrawala\n\nControlNet paper from Stanford University\nhttps://arxiv.org/abs/2302.05543",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#google-colab-notebooks",
    "href": "src/01/resources.html#google-colab-notebooks",
    "title": "Resources",
    "section": "Google Colab Notebooks",
    "text": "Google Colab Notebooks\nAll demo notebooks from the presentation:\n\nGPT-2 Demo\n\nhttps://colab.research.google.com/drive/1H0NZYU-U3BmTS2bUqrVw9ssuFsp5mr8n?usp=sharing\n\nOpenAI SDK/API Call\n\nhttps://colab.research.google.com/drive/1ktzdxJUcHQ6yEY8tJSeikTXjVgwG9DLc?usp=sharing\n\nOpenRouter Demo\n\nhttps://colab.research.google.com/drive/16HB_vtFl5QkSrkGhb5onBgC0Cjx59K9S?usp=sharing\n\nGemma 3 1B via Transformers\n\nhttps://colab.research.google.com/drive/1-2515Fku_5vtgUx7zRF0o0QdZgOTGZk2?usp=sharing\n\nDiffusion Text-to-Image\n\nhttps://colab.research.google.com/drive/1YZYskU2laocx2dNpyYSjcTZZuVQBb4xX?usp=sharing\n\nDiffusion Image-to-Image\n\nhttps://colab.research.google.com/drive/1MaQ-WvYOVF_wb-HpQNNsVxemG4xt6YGV?usp=sharing\n\nControlNet Human Pose\n\nhttps://colab.research.google.com/drive/1uW5ix9dnHTMsoeUt38H936zYw5eYUk3u?usp=sharing",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#development-platforms-tools",
    "href": "src/01/resources.html#development-platforms-tools",
    "title": "Resources",
    "section": "Development Platforms & Tools",
    "text": "Development Platforms & Tools\n\nGoogle Colab\n\nWebsite: https://colab.research.google.com\nSignup: https://colab.research.google.com/signup\nFree Jupyter notebook environment with GPU access\nTiers: Free (T4, 16GB VRAM), Pro (V100, 16GB), Pro+ (A100, 40GB)\n\n\n\nHugging Face\n\nMain Site: https://huggingface.co\nModels: https://huggingface.co/models\nTransformers Docs: https://huggingface.co/docs/transformers\nGemma 3 1B Model: https://huggingface.co/google/gemma-3-1b-it\nThe GitHub of AI models - explore, download, and share models and datasets",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#api-providers-model-access",
    "href": "src/01/resources.html#api-providers-model-access",
    "title": "Resources",
    "section": "API Providers & Model Access",
    "text": "API Providers & Model Access\n\nOpenAI\n\nWebsite: https://openai.com\nAPI Docs: https://platform.openai.com/docs\nProvides access to GPT models (GPT-3.5, GPT-4, GPT-4o, etc.)\nChat Completions API launched March 2023\n\n\n\nAnthropic\n\nWebsite: https://anthropic.com\nAPI Docs: https://docs.anthropic.com\nClaude models (Sonnet, Opus, Haiku)\nFounded January 2021 by former OpenAI researchers\n\n\n\nOpenRouter\n\nWebsite: https://openrouter.ai\nUnified API to hundreds of AI models through a single endpoint\nCompatible with OpenAI‚Äôs Chat Completions API format\nAccess to OpenAI, Claude, Gemini, Grok, Nova, Llama, DeepSeek, Qwen, and more",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#local-model-hosting-tools",
    "href": "src/01/resources.html#local-model-hosting-tools",
    "title": "Resources",
    "section": "Local Model Hosting Tools",
    "text": "Local Model Hosting Tools\n\nLM Studio\n\nWebsite: https://lmstudio.ai\nDesktop application for running LLMs locally\nSupports GGUF quantized models\nBuilt on llama.cpp\n\n\n\nOllama\n\nWebsite: https://ollama.ai\nCommand-line tool for running LLMs locally\nSimple model management and deployment\n\n\n\nllama.cpp\n\nGitHub: https://github.com/ggerganov/llama.cpp\nPure C/C++ implementation of LLM inference\nFoundation for GGUF quantization format\nPowers many local hosting tools",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#major-language-models",
    "href": "src/01/resources.html#major-language-models",
    "title": "Resources",
    "section": "Major Language Models",
    "text": "Major Language Models\n\nGPT Series (OpenAI)\n\nGPT-1 (June 2018): 117M parameters\nGPT-2 (Feb 2019): 1.5B parameters\nGPT-3 (May 2020): 175B parameters\nGPT-3.5 / ChatGPT (Nov 2022): RLHF-tuned\nGPT-4 series (2023+): Multimodal capabilities\n\n\n\nLlama Series (Meta)\n\nLlama 1 (Feb 2023): 7B-65B parameters, researcher access only\nLlama 2 (Jul 2023): First open-weights commercial license\nLlama 3 series (2024+): Improved performance and scale\n\n\n\nGemma Series (Google)\n\nGemma 3 1B-IT: https://huggingface.co/google/gemma-3-1b-it\nAvailable in 1B, 4B, 12B, and 27B sizes\nInstruction-tuned variants for chat applications\n\n\n\nOther Notable Models\n\nMistral: Open-weight models from Mistral AI\nOLMo: Fully open-source model from AI2 (Allen Institute for AI)\nDeepSeek, Qwen: Chinese open-weight models",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#image-generation-models-tools",
    "href": "src/01/resources.html#image-generation-models-tools",
    "title": "Resources",
    "section": "Image Generation Models & Tools",
    "text": "Image Generation Models & Tools\n\nStable Diffusion\n\nStability AI: https://stability.ai\nOpen-source text-to-image diffusion models\nTimeline: v1.4 (Aug 2022) ‚Üí v1.5 ‚Üí v2.0/2.1 ‚Üí SDXL (Jul 2023) ‚Üí v3.5 (Jun 2024)\n\n\n\nMidjourney\n\nWebsite: https://midjourney.com\nDiscord-based image generation service\nKnown for exceptional artistic quality\nv5 launched March 2023\n\n\n\nFLUX.1\n\nBlack Forest Labs: https://blackforestlabs.ai\nState-of-the-art open image generation model (2024)\n\n\n\nDALL-E / Sora (OpenAI)\n\nDALL-E: Text-to-image generation\nSora (Feb 2024): Text-to-video up to 60 seconds\n\n\n\nImagen 3 (Google DeepMind)\n\nPhotorealistic image generation\n2024 release",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#image-model-platforms-resources",
    "href": "src/01/resources.html#image-model-platforms-resources",
    "title": "Resources",
    "section": "Image Model Platforms & Resources",
    "text": "Image Model Platforms & Resources\n\nReplicate\n\nWebsite: https://replicate.com\nCloud platform for running AI models via API\nExtensive library of image generation models\n\n\n\nComfyUI\n\nWebsite: https://comfy.org\nNode-based UI for Stable Diffusion workflows\nPowerful tool for complex image generation pipelines\n\n\n\nHugging Face Diffusers\n\nDocs: https://huggingface.co/docs/diffusers\nPython library for diffusion models\nEasy access to text-to-image, image-to-image, and ControlNet",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#key-concepts-techniques",
    "href": "src/01/resources.html#key-concepts-techniques",
    "title": "Resources",
    "section": "Key Concepts & Techniques",
    "text": "Key Concepts & Techniques\n\nRLHF (Reinforcement Learning from Human Feedback)\n\nTechnique for fine-tuning models to follow instructions\nUsed in InstructGPT, ChatGPT, and Claude\nHuman raters rank model responses to train a reward model\n\n\n\nQuantization\n\nGGUF Format: GPT-Generated Unified Format\n\nSingle-file architecture supporting 2-bit to 8-bit quantization\nDeveloped by llama.cpp community\n\nMLX Format: Apple‚Äôs ML framework for Apple Silicon\n\nSupports 4-bit and 8-bit quantization\nReleased late 2023\n\n\n\n\nControlNet\n\nAdd-on models for Stable Diffusion providing precise spatial control\nConditioning types: pose, edges, depth, normal maps, segmentation, scribbles\nPublished February 2023 by Stanford researchers\n\n\n\nDiffusion Models\n\nTwo-stage process inspired by thermodynamics\nTraining: Learn to predict noise added to images\nInference: Start with random noise, iteratively denoise guided by text prompt",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#additional-learning-resources",
    "href": "src/01/resources.html#additional-learning-resources",
    "title": "Resources",
    "section": "Additional Learning Resources",
    "text": "Additional Learning Resources\n\nAPI Documentation\n\nOpenAI Chat Completions: https://platform.openai.com/docs/api-reference/chat\nAnthropic Messages API: https://docs.anthropic.com/claude/reference/messages_post\n\n\n\nModel Hubs\n\nHugging Face Model Hub: Browse and download thousands of models\nCivitAI: https://civitai.com - Community for Stable Diffusion models\n\n\n\nCommunities\n\nHugging Face Forums: https://discuss.huggingface.co\nr/LocalLLaMA: Reddit community for running models locally\nr/StableDiffusion: Reddit community for image generation",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/01/resources.html#model-parameter-comparison",
    "href": "src/01/resources.html#model-parameter-comparison",
    "title": "Resources",
    "section": "Model Parameter Comparison",
    "text": "Model Parameter Comparison\n\nText Models (Approximate Sizes)\n\n1B parameters: ~2GB\n4B parameters: ~8.6GB\n12B parameters: ~23GB\n70B parameters: ~140GB (full precision)\n175B parameters (GPT-3): ~350GB (full precision)\n\nNote: Quantization can reduce these sizes by 50-75% with minimal quality loss",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html",
    "href": "src/00/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This document contains all the external resources, links, and references mentioned in the ‚ÄúIntroducing AI Agents‚Äù lecture.",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#demo-agent",
    "href": "src/00/resources.html#demo-agent",
    "title": "Resources",
    "section": "Demo Agent",
    "text": "Demo Agent\n\nCampus Agent Demo: https://simonguest-campus-agent.hf.space/\n\nInteractive demo of the DigiPen Campus Assistant agent\n\nCampus Agent Source Code: https://github.com/simonguest/CSP-400/blob/main/demos/00/campus-agent\n\nComplete source code for the DigiPen Campus Assistant demo",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#development-tools-and-platforms",
    "href": "src/00/resources.html#development-tools-and-platforms",
    "title": "Resources",
    "section": "Development Tools and Platforms",
    "text": "Development Tools and Platforms\n\nUI Frameworks\n\nGradio: https://www.gradio.app/\n\nPython library for building machine learning web interfaces\n\nGradio Documentation: https://gradio.app\n\nOfficial Gradio documentation and tutorials\n\nHugging Face Spaces: https://huggingface.co/spaces\n\nHost and share Gradio-based apps on Hugging Face\n\n\n\n\nAgent Frameworks\n\nOpenAI\n\nOpenAI Agents SDK (Python): https://openai.github.io/openai-agents-python/\n\nOfficial OpenAI agents framework for Python\n\nOpenAI Agents Visualization: https://openai.github.io/openai-agents-python/visualization/#showing-the-graph\n\nTool for visualizing agent graphs and interactions\n\n\n\n\nLangChain\n\nLangGraph: https://langchain-ai.github.io/langgraph/\n\nPython framework for building stateful, multi-actor applications with LLMs\n\n\n\n\nCrew AI\n\nCrew.ai: https://github.com/crewaiinc/crewai\n\nFramework for orchestrating role-playing, autonomous AI agents\n\n\n\n\nMicrosoft\n\nAutoGen: https://microsoft.github.io/autogen/stable/\n\nMicrosoft‚Äôs framework for building conversational AI systems\n\nMicrosoft Semantic Kernel: https://github.com/microsoft/semantic-kernel\n\nSDK for integrating AI services with conventional programming languages",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#protocols-and-standards",
    "href": "src/00/resources.html#protocols-and-standards",
    "title": "Resources",
    "section": "Protocols and Standards",
    "text": "Protocols and Standards\n\nModel Context Protocol (MCP): https://modelcontextprototcol.io\n\nStandardized protocol for connecting AI models with external tools and data sources",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#license-information",
    "href": "src/00/resources.html#license-information",
    "title": "Resources",
    "section": "License Information",
    "text": "License Information\nMost of the agent frameworks mentioned use the MIT License, making them suitable for both academic and commercial use: - OpenAI Agents SDK - LangGraph - Crew.ai - Microsoft AutoGen - Microsoft Semantic Kernel",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#sidebars",
    "href": "src/00/resources.html#sidebars",
    "title": "Resources",
    "section": "Sidebars",
    "text": "Sidebars\n\nSimple Made Easy: https://www.youtube.com/watch?v=SxdOUGdseq4\n\nRich Hickey‚Äôs talk on the difference between ‚Äúeasy‚Äù and ‚Äúsimple‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "src/00/resources.html#articles-and-industry-reports",
    "href": "src/00/resources.html#articles-and-industry-reports",
    "title": "Resources",
    "section": "Articles and Industry Reports",
    "text": "Articles and Industry Reports\n\nWorld Economic Forum - Cognitive Enterprise: https://www.weforum.org/stories/2025/06/cognitive-enterprise-agentic-business-revolution/\n\nArticle on the agentic business revolution\n\nCRN - Hottest Agentic AI Tools: https://www.crn.com/news/ai/2025/10-hottest-agentic-ai-tools-and-agents-of-2025-so-far\n\nOverview of the top agentic AI tools of 2025\n\nGartner Press Release: https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027\n\nGartner‚Äôs predictions about agentic AI project success rates\n\nAnthropic - Building Effective Agents: https://www.anthropic.com/engineering/building-effective-agents\n\nEngineering guide on building effective AI agents and patterns\n\nE2B - AI Agents Landscape: https://e2b.dev\n\nOverview of the AI agents ecosystem and available frameworks",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Resources"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSP-400",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CSP-400 course.",
    "crumbs": [
      "CSP-400"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "CSP-400",
    "section": "",
    "text": "This site contains a collection of slides, code, and other resources for lectures held within the 25/26 CSP-400 course.",
    "crumbs": [
      "CSP-400"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#before-we-get-started",
    "href": "src/00/ai_agents.html#before-we-get-started",
    "title": "Introducing AI Agents",
    "section": "Before We Get Started",
    "text": "Before We Get Started\n\nHow many have used ChatGPT/Claude?\nHow many have built their own chatbot and/or used the OpenAI/Claude APIs?\nHow many have created something beyond this?",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#chatbots-are-not-that-smart",
    "href": "src/00/ai_agents.html#chatbots-are-not-that-smart",
    "title": "Introducing AI Agents",
    "section": "Chatbots are not that Smart",
    "text": "Chatbots are not that Smart\nThere are many limitations‚Ä¶\n\nNeeds constant human input every turn; No ability to plan beyond a single interaction\nSingle model with single context\nNo interaction with external systems",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent",
    "href": "src/00/ai_agents.html#what-is-an-agent",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.youtube.com/watch?v=bwXaJXgezf4",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent-1",
    "href": "src/00/ai_agents.html#what-is-an-agent-1",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.weforum.org/stories/2025/06/cognitive-enterprise-agentic-business-revolution/",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent-2",
    "href": "src/00/ai_agents.html#what-is-an-agent-2",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.crn.com/news/ai/2025/10-hottest-agentic-ai-tools-and-agents-of-2025-so-far",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent-3",
    "href": "src/00/ai_agents.html#what-is-an-agent-3",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\n\nSource: https://www.gartner.com/en/newsroom/press-releases/2025-06-25-gartner-predicts-over-40-percent-of-agentic-ai-projects-will-be-canceled-by-end-of-2027",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-is-an-agent-4",
    "href": "src/00/ai_agents.html#what-is-an-agent-4",
    "title": "Introducing AI Agents",
    "section": "What is an Agent?",
    "text": "What is an Agent?\nImagine a DigiPen Campus Assistant: An agent that can help you navigate anything and everything at DigiPen!\n\n‚ÄúWhere can I find the ‚ÄòHopper‚Äô room?‚Äù\n‚ÄúCan you tell me more about FLM201?‚Äù\n‚ÄúOh, and what‚Äôs today‚Äôs vegetarian option at the Bytes Cafe?‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Planners\n\n\nAgents are driven by goals\nAnd they can put together a plan for the steps to complete that goal.\n\n‚ÄúFirst, I will discover where course information is located‚Äù\n‚ÄúThen I will search for any courses that reference FLM201‚Äù\n‚ÄúThen I summarize all of the key points for the student‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents-1",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents-1",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Autonomous\n\n\nAgents can then go off and execute the plan, independent of human input\nThe concept of ‚Äúhuman in the loop‚Äù still applies for confirmation\n\ne.g.¬†‚ÄúDo you really want to place this order at the Bytes Cafe?‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents-2",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents-2",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Reactive\n\n\nAgents can change mid-course depending on what they find and/or the environment.\n\ne.g.¬†‚ÄúI couldn‚Äôt find any course information on FLM201. I‚Äôm going to whether there are other 200-level FLM courses before responding to the student.‚Äù",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents-3",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents-3",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents have Persistence\n\n\nAgents often have memory systems beyond the current conversation\nBroadly classified as short and long term memory\n\nShort term memory could be the options at the Bytes Cafe\nLong term memory could be your food preferences",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#five-characteristics-of-agents-4",
    "href": "src/00/ai_agents.html#five-characteristics-of-agents-4",
    "title": "Introducing AI Agents",
    "section": "Five Characteristics of Agents",
    "text": "Five Characteristics of Agents\n\nAgents are Interactive\n\n\nAgents can delegate to other agents for complex tasks\n\n(Or for tasks where other agents are better suited for.)\ne.g., Campus Agent -&gt; delegating to a Course Agent\n\nAgents can also be given access to external tools\n\ne.g., File search, Web search, access to the Bytes Cafe API",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#lets-try-this-1",
    "href": "src/00/ai_agents.html#lets-try-this-1",
    "title": "Introducing AI Agents",
    "section": "Let‚Äôs Try This!",
    "text": "Let‚Äôs Try This!\n\n\n\n\nhttps://simonguest-campus-agent.hf.space/",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#lets-try-this-2",
    "href": "src/00/ai_agents.html#lets-try-this-2",
    "title": "Introducing AI Agents",
    "section": "Let‚Äôs Try This!",
    "text": "Let‚Äôs Try This!\n\n\n\n\nhttps://simonguest-campus-agent.hf.space/",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#observations-and-questions",
    "href": "src/00/ai_agents.html#observations-and-questions",
    "title": "Introducing AI Agents",
    "section": "Observations and Questions",
    "text": "Observations and Questions\nGet into groups of 2 or 3\n\nQ: What worked? What surprised you?\nQ: What didn‚Äôt work? Where did the agent fail?\nQ: What other examples of agents can you think of?",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#other-examples-for-agents",
    "href": "src/00/ai_agents.html#other-examples-for-agents",
    "title": "Introducing AI Agents",
    "section": "Other Examples for Agents",
    "text": "Other Examples for Agents\n\nCustomer service agent\nTravel booking agent\nResearch assistant\nCode generation agent (very popular right now)\nAgents within games\n\nTraditional NPCs are pre-written dialogue trees\nWhereas agents can be more independent within the game environment",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#gradio-based-ui",
    "href": "src/00/ai_agents.html#gradio-based-ui",
    "title": "Introducing AI Agents",
    "section": "Gradio-Based UI",
    "text": "Gradio-Based UI\n\nSource: https://www.gradio.app/",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#gradio-based-ui-1",
    "href": "src/00/ai_agents.html#gradio-based-ui-1",
    "title": "Introducing AI Agents",
    "section": "Gradio-Based UI",
    "text": "Gradio-Based UI\nimport time\nimport gradio as gr\n\ndef slow_echo(message, history):\n    for i in range(len(message)):\n        time.sleep(0.05)\n        yield \"You typed: \" + message[: i + 1]\n\ndemo = gr.ChatInterface(\n    slow_echo,\n    flagging_mode=\"manual\",\n    flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n    save_history=True,\n)\n\nif __name__ == \"__main__\":\n    demo.launch()",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#gradio-based-ui-2",
    "href": "src/00/ai_agents.html#gradio-based-ui-2",
    "title": "Introducing AI Agents",
    "section": "Gradio-Based UI",
    "text": "Gradio-Based UI\n\nSource: https://gradio.app",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#hosted-on-hugging-face-spaces",
    "href": "src/00/ai_agents.html#hosted-on-hugging-face-spaces",
    "title": "Introducing AI Agents",
    "section": "Hosted on Hugging Face Spaces",
    "text": "Hosted on Hugging Face Spaces\n\nSource: https://huggingface.co/spaces",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#agent-structure",
    "href": "src/00/ai_agents.html#agent-structure",
    "title": "Introducing AI Agents",
    "section": "Agent Structure",
    "text": "Agent Structure\n\nAgents and tools for the DigiPen Campus Assistant",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#so-many-agent-frameworks",
    "href": "src/00/ai_agents.html#so-many-agent-frameworks",
    "title": "Introducing AI Agents",
    "section": "So Many Agent Frameworks‚Ä¶",
    "text": "So Many Agent Frameworks‚Ä¶\n\nSource: https://e2b.dev",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-agents-sdk",
    "href": "src/00/ai_agents.html#openai-agents-sdk",
    "title": "Introducing AI Agents",
    "section": "OpenAI Agents SDK",
    "text": "OpenAI Agents SDK\n\nUsed for our demo\nhttps://openai.github.io/openai-agents-python/\nPython and JavaScript/TypeScript\nMIT License\nWorks with models that support OpenAI‚Äôs Responses API",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#langgraph",
    "href": "src/00/ai_agents.html#langgraph",
    "title": "Introducing AI Agents",
    "section": "LangGraph",
    "text": "LangGraph\n\nhttps://langchain-ai.github.io/langgraph/\nPython only\nMIT License",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#crew.ai",
    "href": "src/00/ai_agents.html#crew.ai",
    "title": "Introducing AI Agents",
    "section": "Crew.ai",
    "text": "Crew.ai\n\nhttps://github.com/crewaiinc/crewai\nPython only\nMIT License",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#microsoft",
    "href": "src/00/ai_agents.html#microsoft",
    "title": "Introducing AI Agents",
    "section": "Microsoft",
    "text": "Microsoft\n\nAutoGen\n\nhttps://microsoft.github.io/autogen/stable/\nPython (.NET coming soon)\nMIT License\n\nMicrosoft Semantic Kernel\n\nhttps://github.com/microsoft/semantic-kernel\nPython, .NET, Java\nMIT License",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-should-you-be-considering",
    "href": "src/00/ai_agents.html#what-should-you-be-considering",
    "title": "Introducing AI Agents",
    "section": "What Should You Be Considering?",
    "text": "What Should You Be Considering?\n\nQuestion: If most agents run in the browser, why are many of the agent frameworks written in Python? ü§î",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#what-should-you-be-considering-1",
    "href": "src/00/ai_agents.html#what-should-you-be-considering-1",
    "title": "Introducing AI Agents",
    "section": "What Should You Be Considering?",
    "text": "What Should You Be Considering?\n\nWhere will your agent run?\n\nServer or client? On the web? In game?\nThis will likely determine the language\nHow will you manage API keys?\n\nSupport\n\nWill this agent framework be around in 3-5 years?\nIs there a cost/hosting component to it?\n\nEasy vs.¬†Simple\n\nIs your choice ‚Äúeasy‚Äù or ‚Äúsimple‚Äù?",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#sidebar-easy-vs.-simple",
    "href": "src/00/ai_agents.html#sidebar-easy-vs.-simple",
    "title": "Introducing AI Agents",
    "section": "Sidebar: Easy vs.¬†Simple",
    "text": "Sidebar: Easy vs.¬†Simple\n\nhttps://www.youtube.com/watch?v=SxdOUGdseq4",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-agents-sdk-python",
    "href": "src/00/ai_agents.html#openai-agents-sdk-python",
    "title": "Introducing AI Agents",
    "section": "OpenAI Agents SDK (Python)",
    "text": "OpenAI Agents SDK (Python)\nCreating and running a new agent\nfrom agents import Agent, Runner\n\nagent = Agent(\n    name=\"DigiPen Campus Assistant\",\n    instructions=\"You are a helpful campus assistant that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)\n\nmessages.append({\"role\": \"user\", \"content\": user_msg})\nresult = Runner.run_streamed(agent, messages)",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-agents-sdk-python-1",
    "href": "src/00/ai_agents.html#openai-agents-sdk-python-1",
    "title": "Introducing AI Agents",
    "section": "OpenAI Agents SDK (Python)",
    "text": "OpenAI Agents SDK (Python)\nSpecifying handoffs\nfrom agents import Agent, Runner\n\nagent = Agent(\n    name=\"DigiPen Campus Assistant\",\n    instructions=\"You are a helpful campus assistant that can plan and execute tasks for students at DigiPen. Please be concise and accurate in handing off tasks to other agents as needed.\",\n    handoffs=[building_agent, course_agent, handbook_agent, cafe_agent],\n)\n\nmessages.append({\"role\": \"user\", \"content\": user_msg})\nresult = Runner.run_streamed(agent, messages)",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#why-multiple-agents",
    "href": "src/00/ai_agents.html#why-multiple-agents",
    "title": "Introducing AI Agents",
    "section": "Why Multiple Agents?",
    "text": "Why Multiple Agents?\n\nContext window limitations\nEach agent can have a different system prompt (instructions)\nEach agent can have a different underlying model\n\nSpecialized models (e.g., a vision encoder)\nOr to blend cost\n\nMakes tool separation cleaner and more accurate",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#example-of-multiple-agents",
    "href": "src/00/ai_agents.html#example-of-multiple-agents",
    "title": "Introducing AI Agents",
    "section": "Example of Multiple Agents",
    "text": "Example of Multiple Agents\n\nCode generation\n\nAgents for ‚Äòarchitect‚Äô, code writer, tester, debugger, etc.\n\nContent generation\n\nAgent to create content, other agents to generate images, translate content, etc.\n\nTravel booking\n\nAgent to book flights, hotels, cars, etc. for packages",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#patterns-for-agents",
    "href": "src/00/ai_agents.html#patterns-for-agents",
    "title": "Introducing AI Agents",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nAs you get deeper into building agents, patterns start to emerge\n\ne.g., router (which is what we used in our demo), orchestrator (using other agents as tools), parallel agents",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#patterns-for-agents-1",
    "href": "src/00/ai_agents.html#patterns-for-agents-1",
    "title": "Introducing AI Agents",
    "section": "Patterns for Agents",
    "text": "Patterns for Agents\n\nSource: https://www.anthropic.com/engineering/building-effective-agents",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#why-do-agents-need-tools",
    "href": "src/00/ai_agents.html#why-do-agents-need-tools",
    "title": "Introducing AI Agents",
    "section": "Why Do Agents Need Tools?",
    "text": "Why Do Agents Need Tools?\n\nThe scope of the agents ability is contained within the model\nTools enable the agent to reach out to systems beyond the model\nExamples\n\nRead a file from disk or search the web (built in)\nCalculator (because LLMs aren‚Äôt great at math)\nCode interpreter (running code on the fly)\nWhat potential tools can you think of? ü§î",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-tool-calling",
    "href": "src/00/ai_agents.html#openai-tool-calling",
    "title": "Introducing AI Agents",
    "section": "OpenAI Tool Calling",
    "text": "OpenAI Tool Calling\n\nIntroduced by OpenAI in June 2023\nOriginally called Function Calling\nModels are fine-tuned to return a structured function_call JSON object, specifying which function to call and with what arguments.\nTools are provided as functions\nOption for the LLM to decide when to call the tool (always, never, auto)",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-tool-calling-1",
    "href": "src/00/ai_agents.html#openai-tool-calling-1",
    "title": "Introducing AI Agents",
    "section": "OpenAI Tool Calling",
    "text": "OpenAI Tool Calling\nfrom agents import Agent, function_tool\n\n@function_tool\ndef get_bytes_cafe_menu(date: str) -&gt; any:\n    \"\"\"Returns the menu for the Bytes Cafe for the date provided.\"\"\"\n    return { f\"{date}\": {\n            \"daily byte\": {\n                \"name\": \"Steak Quesadilla\", \"price\": 12, \"description\": \"Flank steak, mixed cheese in a flour tortilla served with air fried potatoes, sour cream and salsa\",\n            },\n        } }\n\ncafe_agent = Agent(\n    name=\"Cafe Agent\",\n    instructions=\"You help students locate and provide information about the Bytes Cafe.\",\n    tools=[\n        get_bytes_cafe_menu,\n    ])",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#openai-tool-calling-2",
    "href": "src/00/ai_agents.html#openai-tool-calling-2",
    "title": "Introducing AI Agents",
    "section": "OpenAI Tool Calling",
    "text": "OpenAI Tool Calling\nfrom agents import Agent, FileSearchTool\n\nVECTOR_STORE_ID = \"vs_6896d8c959008191981d645850b42313\"\n\nbuilding_agent = Agent(\n    name=\"Building Agent\",\n    instructions=\"You help students locate and provide information about buildings and rooms on campus. Be descriptive when giving locations.\",\n    tools=[\n        FileSearchTool(\n            max_num_results=3,\n            vector_store_ids=[VECTOR_STORE_ID],\n            include_search_results=True,\n        )\n    ],\n)",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#mcp-model-context-protocol",
    "href": "src/00/ai_agents.html#mcp-model-context-protocol",
    "title": "Introducing AI Agents",
    "section": "MCP (Model Context Protocol)",
    "text": "MCP (Model Context Protocol)\n\nOpenAI‚Äôs tool/function calling works if you have local access to the tools\nBut how about external systems?\n\ne.g., Access to calendar, other systems, hardware, etc.\n\nYou could build local tools to call APIs\nMCP: Model Context Protocol\nStandardized tooling interface, released by Anthropic in March 2024\nMore details: https://modelcontextprototcol.io",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#using-an-agent-for-your-project",
    "href": "src/00/ai_agents.html#using-an-agent-for-your-project",
    "title": "Introducing AI Agents",
    "section": "Using an Agent for Your Project",
    "text": "Using an Agent for Your Project\n\nCould your project benefit from agents?\nOne or more agents?\nHow would agents interact with the user?\nWhat might be some of the challenges to overcome?",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#resources-12",
    "href": "src/00/ai_agents.html#resources-12",
    "title": "Introducing AI Agents",
    "section": "Resources (1/2)",
    "text": "Resources (1/2)\n\nThis slide deck, resources, links, everything:\n\nhttps://simonguest.github.io/CSP-400",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/00/ai_agents.html#resources-22",
    "href": "src/00/ai_agents.html#resources-22",
    "title": "Introducing AI Agents",
    "section": "Resources (2/2)",
    "text": "Resources (2/2)\n\nCampus agent code:\n\nhttps://github.com/simonguest/CSP-400/tree/main/demos/00/campus-agent\nClone the repo, get it running locally\nModify by adding a new agent to it\nCreate your own use case/agent in the Gradio interface",
    "crumbs": [
      "Lecture: Introducing AI Agents",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#recap-of-last-weeks-lecture",
    "href": "src/01/exploring_genai.html#recap-of-last-weeks-lecture",
    "title": "Exploring Generative AI Models",
    "section": "Recap of Last Week‚Äôs Lecture",
    "text": "Recap of Last Week‚Äôs Lecture\n\nIntroduced AI Agents, their uses, how to create them\nAbout 50% had used an API to call AI Model\nOne or two beyond this",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#this-week",
    "href": "src/01/exploring_genai.html#this-week",
    "title": "Exploring Generative AI Models",
    "section": "This Week",
    "text": "This Week\n\nTwo Part Lecture on AI Models\n\nThis Week\n\nExplore text-to-text models\nModel evolution, API access, running locally\nFive Demos!\n\nNext Week\n\nExplore image models\nDiffuser, ControlNet, and VLMs\nMore Demos!",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#a-brief-history-of-transformer-models-1",
    "href": "src/01/exploring_genai.html#a-brief-history-of-transformer-models-1",
    "title": "Exploring Generative AI Models",
    "section": "A Brief History of Transformer Models",
    "text": "A Brief History of Transformer Models\n\n\n\n\n\ntimeline\n    June 2017 : Google researchers publish \"Attention is all you need\" paper [1]\n              : Introduces self-attention mechanism and transformer architecture\n              : Eliminates the need for recurrent neural networks in sequence processing\n\n    June 2018 : OpenAI releases GPT-1\n              : 117M parameters\n              : Demonstrates pre-training on large text corpora followed by fine-tuning works effectively\n\n    Feb 2019 : OpenAI releases GPT-2\n             : 1.5B parameters\n             : Initially withheld full model due to concerns about misuse\n             : Demonstrates impressive text generation capabilities with minimal fine-tuning\n\n    May 2020 : OpenAI releases GPT-3\n             : 175B parameters\n             : Demonstrates strong few-shot learning capabilities\n             : Marks a significant leap in model capabilities and scale\n\n    June 2020 : GPT-3 available through OpenAI API\n              : Still a completion model, not instruction-tuned\n\n\n\n\n\n\n[1] Vaswani et al. (2017)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#completion-vs.-instruction-tuned",
    "href": "src/01/exploring_genai.html#completion-vs.-instruction-tuned",
    "title": "Exploring Generative AI Models",
    "section": "Completion vs.¬†Instruction-Tuned",
    "text": "Completion vs.¬†Instruction-Tuned\n\nCompletion Model just predicts the next token\n\nInput prompt: Mary had a little\nMax total tokens: 50\nTemperature: 0 - 1.0\ntop_k: consider only the top k tokens in the response\ntop_p: Nucleus sampling (probability cut off - 0 and 1.0)\n\nOutput\n\nMary had a little lamb, its fleece was white as snow... (up to max tokens)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#completion-vs.-instruction-tuned-1",
    "href": "src/01/exploring_genai.html#completion-vs.-instruction-tuned-1",
    "title": "Exploring Generative AI Models",
    "section": "Completion vs.¬†Instruction-Tuned",
    "text": "Completion vs.¬†Instruction-Tuned\n\nYou can‚Äôt really converse with it\nWhat is the capital of France? (max tokens = 50)\nWhat is the capital of France? Paris. What is the capital of Spain? Madrid. What is the capital of\nBut it‚Äôs the foundation of today‚Äôs text models, and fun to play with‚Ä¶",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#introducing-google-colab",
    "href": "src/01/exploring_genai.html#introducing-google-colab",
    "title": "Exploring Generative AI Models",
    "section": "Introducing Google Colab",
    "text": "Introducing Google Colab\n\nSource: https://colab.research.google.com/signup",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-gpt-2",
    "href": "src/01/exploring_genai.html#demo-gpt-2",
    "title": "Exploring Generative AI Models",
    "section": "Demo: GPT-2",
    "text": "Demo: GPT-2\nGPT-2.ipynb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#instruction-tuned-models",
    "href": "src/01/exploring_genai.html#instruction-tuned-models",
    "title": "Exploring Generative AI Models",
    "section": "Instruction-Tuned Models",
    "text": "Instruction-Tuned Models\n\nSupervised Fine-Tuning\n\nLarge datasets of questions/answers, tasks/completions, demonstrating helpful assistant behavior\n\nRLHF (Reinforcement Learning from Human Feedback)\n\nHuman raters rank different model responses, training a reward model\n\nChat Templates\n\nStructured formats to distinguish speakers in a dialog: Typically system, user, and assistant",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#a-brief-history-of-transformer-models-2",
    "href": "src/01/exploring_genai.html#a-brief-history-of-transformer-models-2",
    "title": "Exploring Generative AI Models",
    "section": "A Brief History of Transformer Models",
    "text": "A Brief History of Transformer Models\n\n\n\n\n\ntimeline\n    2021 : InstructGPT Development\n          : Built on GPT-3 with RLHF fine-tuning\n          : Trained to follow instructions and understand user intent\n          : Key innovation enabling ChatGPT\n    \n    Jan 2021 : Anthropic Founded\n             : Founded by Dario & Daniela Amodei with ~7 senior OpenAI employees\n            : Dario led GPT-2/3 development and co-invented RLHF\n\n    Nov 2022 : ChatGPT Launch\n                  : Built on GPT-3.5 using RLHF\n                  : 1M+ users in 5 days\n                  : Sparked widespread interest in generative AI\n\n    Feb 2023 : Llama 1 Released\n                  : Meta's LLaMA (7B, 13B, 33B, 65B parameters)\n                  : 13B model exceeded GPT-3 (175B) on most benchmarks\n                  : Limited researcher access\n                  : Text completion only (Alpaca fine-tune added instructions)\n\n    Jul 2023 : Llama 2 Released\n              : Available in 7B, 13B, 70B sizes\n              : Trained on 40% more data than Llama 1\n              : First open-weights Llama for commercial use",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#closed-vs.-open-models",
    "href": "src/01/exploring_genai.html#closed-vs.-open-models",
    "title": "Exploring Generative AI Models",
    "section": "Closed vs.¬†Open Models",
    "text": "Closed vs.¬†Open Models\n\nClosed Source: Hosted models; no ability to inspect the weights of the models. Accessed via an API (or UI).\n\nExamples: OpenAI GPT-5, Claude Sonnet 4.5\n\nOpen Weight: Model files with pretrained weights, but no training data. Host on your own hardware.\n\nExamples: Meta‚Äôs Llama (and derivatives), Gemma\n\nOpen Source: Models with access to the training data set. Create from scratch.\n\nExamples: OLMo from AI2",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#calling-models-via-apis-1",
    "href": "src/01/exploring_genai.html#calling-models-via-apis-1",
    "title": "Exploring Generative AI Models",
    "section": "Calling Models via APIs",
    "text": "Calling Models via APIs\n\nHTTP-based APIs\n\nClient makes HTTP API calls to invoke/access the model\n(Normally use an SDK to wrap the HTTP API calls)\nClient passes Authorization token as part of the call\nDefault way of accessing OpenAI, Claude, other large, closed-source models",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-openai-sdkapi-call",
    "href": "src/01/exploring_genai.html#demo-openai-sdkapi-call",
    "title": "Exploring Generative AI Models",
    "section": "Demo: OpenAI SDK/API call",
    "text": "Demo: OpenAI SDK/API call\nOpenAI.ipynb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#openais-chat-completions-api",
    "href": "src/01/exploring_genai.html#openais-chat-completions-api",
    "title": "Exploring Generative AI Models",
    "section": "OpenAI‚Äôs Chat Completions API",
    "text": "OpenAI‚Äôs Chat Completions API\n\nDebuted in March 2023, together with the ChatGPT API\nStructure\n\nMessages array (system, assistant, user)\nStreaming support (using SSE - Server Side Events)\nFunction calling (added mid-2023)\nStructured output (added Aug 2024)\n\nWidespread Adoption\n\nAnthropic, Azure, TogetherAI\nLocal hosting: vLLM, LM Studio",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#openais-chat-completions-api-1",
    "href": "src/01/exploring_genai.html#openais-chat-completions-api-1",
    "title": "Exploring Generative AI Models",
    "section": "OpenAI‚Äôs Chat Completions API",
    "text": "OpenAI‚Äôs Chat Completions API\nHow do we consume different models from multiple providers?\n\nIntroducing OpenRouter (https://openrouter.ai)\n\nA unified API to hundreds of AI models through a single endpoint\n(Using OpenAI‚Äôs Chat Completion API)\nOpenAI, Claude, Gemini, Grok, Nova, Llama, DeepSeek, Qwen, and many others.\nPay per API call, often same cost as the provider",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-openrouter",
    "href": "src/01/exploring_genai.html#demo-openrouter",
    "title": "Exploring Generative AI Models",
    "section": "Demo: OpenRouter",
    "text": "Demo: OpenRouter\nhttps://openrouter.ai\nOpenRouter.ipynb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#downloading-and-running-models-1",
    "href": "src/01/exploring_genai.html#downloading-and-running-models-1",
    "title": "Exploring Generative AI Models",
    "section": "Downloading and Running Models",
    "text": "Downloading and Running Models\n\nSo far, we‚Äôve called hosted models via APIs\nHow about downloading and running models on your own hardware?\n\n(Obviously they need to be open-weight models)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#downloading-and-running-models-2",
    "href": "src/01/exploring_genai.html#downloading-and-running-models-2",
    "title": "Exploring Generative AI Models",
    "section": "Downloading and Running Models",
    "text": "Downloading and Running Models\nWhy would you want to do this?\n\nOffline access to models (no Internet required)\nPotential cost savings (if many API calls and already own hardware)\n\ne.g., running a small model embedded within a game\n\nWant to fine-tune your own model and have the hardware to do it\nDon‚Äôt want others to see the conversations you are having :)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#introducing-hugging-face",
    "href": "src/01/exploring_genai.html#introducing-hugging-face",
    "title": "Exploring Generative AI Models",
    "section": "Introducing Hugging Face",
    "text": "Introducing Hugging Face\n\nSource: https://huggingface.co",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#what-is-hugging-face",
    "href": "src/01/exploring_genai.html#what-is-hugging-face",
    "title": "Exploring Generative AI Models",
    "section": "What is Hugging Face?",
    "text": "What is Hugging Face?\nIt is to AI models what GitHub is to source code\n\nExplore, download models to run on local hardware\nUpload and share your own trained/fine-tuned models and datasets\nCreate ‚ÄúSpaces‚Äù - web-based apps for accessing models",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-exploring-a-model",
    "href": "src/01/exploring_genai.html#demo-exploring-a-model",
    "title": "Exploring Generative AI Models",
    "section": "Demo: Exploring a Model",
    "text": "Demo: Exploring a Model\nGoogle‚Äôs gemma-3-1b-it on Hugging Face",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#hugging-face-transformers",
    "href": "src/01/exploring_genai.html#hugging-face-transformers",
    "title": "Exploring Generative AI Models",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\n\nSource: https://huggingface.co/docs/transformers",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#hugging-face-transformers-1",
    "href": "src/01/exploring_genai.html#hugging-face-transformers-1",
    "title": "Exploring Generative AI Models",
    "section": "Hugging Face Transformers",
    "text": "Hugging Face Transformers\nWhat is the Hugging Face Transformers Library?\n\nOpen-source Python library to provide easy access to using various types of pre-trained transformer models\nBrings together all of the different formats under one interface\n\nDifferent models, vendors, types, chat templates\nDifferent implementations: PyTorch, TensorFlow, JAX\n\nA few lines of code to download and run the model",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-using-hf-transformers-to-download-and-use-gemma-3-1b",
    "href": "src/01/exploring_genai.html#demo-using-hf-transformers-to-download-and-use-gemma-3-1b",
    "title": "Exploring Generative AI Models",
    "section": "Demo: Using HF Transformers to download and use Gemma 3 1B",
    "text": "Demo: Using HF Transformers to download and use Gemma 3 1B\ngemma-3-1b-it via transformers.ipynb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#out-of-vram",
    "href": "src/01/exploring_genai.html#out-of-vram",
    "title": "Exploring Generative AI Models",
    "section": "‚ÄúOut of VRAM‚Äù",
    "text": "‚ÄúOut of VRAM‚Äù\nOne challenge of running models on your own hardware is VRAM\n\nRoughly speaking, the size of the model will determine how much VRAM you need\nGemma 3 models\n\ngemma-3-1b-it = 2Gb\ngemma-3-4b-it = 8.6Gb\ngemma-3-12b-it = 23.37Gb\nQwen3-VL-235B-A22B-Thinking = ~475Gb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#out-of-vram-1",
    "href": "src/01/exploring_genai.html#out-of-vram-1",
    "title": "Exploring Generative AI Models",
    "section": "‚ÄúOut of VRAM‚Äù",
    "text": "‚ÄúOut of VRAM‚Äù\n\nGoogle Colab Tiers\n\nColab Free T4 = 16Gb VRAM (15Gb usable)\nColab Pro V100 = 16Gb VRAM\nColab Pro A100 = 40Gb VRAM\n\nYour Gaming PC\n\nProbably 8Gb VRAM\n\nYour Phone\n\nV-what? :)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#out-of-vram-2",
    "href": "src/01/exploring_genai.html#out-of-vram-2",
    "title": "Exploring Generative AI Models",
    "section": "‚ÄúOut of VRAM‚Äù",
    "text": "‚ÄúOut of VRAM‚Äù\nYou can select smaller models, but they are less accurate / more prone to hallucination.\n\nHow do we fix this?\n\nQuantization",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#quantization",
    "href": "src/01/exploring_genai.html#quantization",
    "title": "Exploring Generative AI Models",
    "section": "Quantization",
    "text": "Quantization\nProcess of reducing the precision of a model‚Äôs weights and activations. For example, 16-bit numbers to 4-bit.\n\nParameter count matters more than precision\n\nA 70B parameter model at 4-bit often beats a 13B model at b16\nThe models knowledge remains largely intact\nOften the extra precision doesn‚Äôt meaningfully improve outputs",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#quantization-formats",
    "href": "src/01/exploring_genai.html#quantization-formats",
    "title": "Exploring Generative AI Models",
    "section": "Quantization Formats",
    "text": "Quantization Formats\nThe llama.cpp project (implementing LLMs in pure C/C++) has driven advancements in quantization\n\nGGUF (GPT-Generated Unified Format)\n\nSingle file architecture\nModel format supporting multiple quantization levels (2-bit through 8-bit) with CPU and GPU handoff\n\nMLX (Apple‚Äôs ML framework and format for Apple Silicon)\n\nDebuted in late 2023\nSupports 4 and 8 bit quantization schemes",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#running-quantized-models",
    "href": "src/01/exploring_genai.html#running-quantized-models",
    "title": "Exploring Generative AI Models",
    "section": "Running Quantized Models",
    "text": "Running Quantized Models\n\nTools built upon llama.cpp\n\nOllama, LM Studio, koboldcpp",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-running-gemma-3-27b-gguf-on-my-laptop",
    "href": "src/01/exploring_genai.html#demo-running-gemma-3-27b-gguf-on-my-laptop",
    "title": "Exploring Generative AI Models",
    "section": "Demo: Running Gemma 3 27B GGUF on my laptop",
    "text": "Demo: Running Gemma 3 27B GGUF on my laptop\nLMStudio: gemma-3-27b-it-qat-q4_0-gguf",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-c-client---gemma-3-27b-local",
    "href": "src/01/exploring_genai.html#demo-c-client---gemma-3-27b-local",
    "title": "Exploring Generative AI Models",
    "section": "Demo: C# Client <-> Gemma 3 27B Local",
    "text": "Demo: C# Client &lt;-&gt; Gemma 3 27B Local\ndemos/01/lmstudio-client/LMStudioClient.csproj",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#hosting-models-in-unity",
    "href": "src/01/exploring_genai.html#hosting-models-in-unity",
    "title": "Exploring Generative AI Models",
    "section": "Hosting Models in Unity",
    "text": "Hosting Models in Unity\n\nDownload the GGUF model locally to Assets/StreamingAssets folder\nUse llama.cpp bindings for C# to host\n\nLLAMASharp: https://github.com/SciSharp/LLamaSharp\n\nUse OpenAI SDK (or similar) as client\nUnity Demo\n\nhttps://github.com/eublefar/LLAMASharpUnityDemo",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#resources-1",
    "href": "src/01/exploring_genai.html#resources-1",
    "title": "Exploring Generative AI Models",
    "section": "Resources",
    "text": "Resources\n\nThis slide deck, resources, links, notebooks, everything:\n\nhttps://simonguest.github.io/CSP-400\n(I‚Äôll also post to the GAM-400 and CSP-300/400 channels)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#images-models-text-to-image",
    "href": "src/01/exploring_genai.html#images-models-text-to-image",
    "title": "Exploring Generative AI Models",
    "section": "Images Models: Text-to-Image",
    "text": "Images Models: Text-to-Image\nText-to-Image: ‚ÄúA photograph of an astronaut riding a horse.‚Äù\n\nBased on a concept called a diffuser\nTwo stage process, inspired by thermodynamics",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-models-diffuser",
    "href": "src/01/exploring_genai.html#image-models-diffuser",
    "title": "Exploring Generative AI Models",
    "section": "Image Models: Diffuser",
    "text": "Image Models: Diffuser\n\nTraining\n\nDuring training, random noise is adding to images in steps\nModel learns to predict what noise was added (forward diffusion process)\n\nInference (process runs in reverse)\n\nStart with pure random noise (known as a seed)\nModel estimates what noise should be removed to create a realistic image\nUsing the text prompt, the model steers the process towards images that match the description",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-diffusion-models-in-2022",
    "href": "src/01/exploring_genai.html#image-diffusion-models-in-2022",
    "title": "Exploring Generative AI Models",
    "section": "Image Diffusion Models in 2022",
    "text": "Image Diffusion Models in 2022\n\n\n\n\n\ntimeline\n  August 2022 : Stable Diffusion v1.4\n              : First open-source high-quality model\n  September 2022 : Stable Diffusion v1.5\n                  : Refined version\n  October 2022 : eDiff-I (NVIDIA)\n                : Ensemble approach\n  November 2022 : Stable Diffusion v2.0/2.1\n                : Higher resolution (768x768)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-diffusion-models-in-2023",
    "href": "src/01/exploring_genai.html#image-diffusion-models-in-2023",
    "title": "Exploring Generative AI Models",
    "section": "Image Diffusion Models in 2023",
    "text": "Image Diffusion Models in 2023\n\n\n\n\n\ntimeline\n  March 2023 : Midjourney v5\n              : Exceptional artistic quality\n  April 2023 : ControlNet\n        : Precise spatial control\n        : AnimateDiff - Video generation\n  July 2023 : SDXL (Stable Diffusion XL)\n            : 1024x1024 native resolution\n  August 2023 : SDXL Turbo\n              : Real-time capable generation",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#image-diffusion-models-in-2024",
    "href": "src/01/exploring_genai.html#image-diffusion-models-in-2024",
    "title": "Exploring Generative AI Models",
    "section": "Image Diffusion Models in 2024",
    "text": "Image Diffusion Models in 2024\n\n\n\n\n\ntimeline\n  February 2024 : Sora (OpenAI)\n                : Text-to-video up to 60 seconds\n                : Stable Diffusion 3\n                : Improved text understanding\n  June 2024 : Stable Diffusion 3.5\n            : Multiple model sizes\n  2024 : FLUX.1 (Black Forest Labs)\n        : State-of-the-art open model\n        : Imagen 3 (Google DeepMind)\n        : Photorealistic quality",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-text-to-image-diffusion-process",
    "href": "src/01/exploring_genai.html#demo-text-to-image-diffusion-process",
    "title": "Exploring Generative AI Models",
    "section": "Demo: Text to Image Diffusion Process",
    "text": "Demo: Text to Image Diffusion Process\nDiffusion Text-to-Image.ipynb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#images-models-image-to-image",
    "href": "src/01/exploring_genai.html#images-models-image-to-image",
    "title": "Exploring Generative AI Models",
    "section": "Images Models: Image-to-Image",
    "text": "Images Models: Image-to-Image\nImage-to-Image: ‚ÄúMake this image different‚Äù\n\nOriginally solved by GAN approaches, but evolved into extension of the diffusion concept\n\nAdd noise to the original image (partial denoising)\nRegenerate it with modifications based on the prompt\nThe original image heavily influences the output structure",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-image-to-image-process",
    "href": "src/01/exploring_genai.html#demo-image-to-image-process",
    "title": "Exploring Generative AI Models",
    "section": "Demo: Image to Image Process",
    "text": "Demo: Image to Image Process\nDiffusion Image-to-Image.ipynb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#controlnet",
    "href": "src/01/exploring_genai.html#controlnet",
    "title": "Exploring Generative AI Models",
    "section": "ControlNet",
    "text": "ControlNet\nText-to-Image and Image-to-Image are interesting, but you don‚Äôt have that much control\n\nExtensive prompts (both positive and negative) can help, but only so far\nFine-tuning techniques, but expensive and risk degrading quality",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#controlnet-1",
    "href": "src/01/exploring_genai.html#controlnet-1",
    "title": "Exploring Generative AI Models",
    "section": "ControlNet",
    "text": "ControlNet\nIntroducing ControlNet\n\nDeveloped by Lvmin Zhang and Maneesh Agrawala at Stanford University\nPublished in February 2023 (Zhang, Rao, and Agrawala 2023)\nControlNet represented a paradigm shift from ‚Äúdescribe what you want‚Äù to ‚Äúshow the structure you want.‚Äù",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#how-controlnet-works",
    "href": "src/01/exploring_genai.html#how-controlnet-works",
    "title": "Exploring Generative AI Models",
    "section": "How ControlNet Works",
    "text": "How ControlNet Works\n\nStable Diffusion‚Äôs U-Net has an encoder and decoder\nCreate a trainable copy of the encoder blocks\nTrain the copy of the encoder alongside the frozen SD model\n\nDuring training: use paired data (e.g., pose skeleton ‚Üí original image)\nDuring inference: both encoders run together\nFeatures from both are combined via zero convolutions\n\nKey: The weights in the original SD model don‚Äôt change\nControlNet is analogous to a ‚ÄúPlug in‚Äù model",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#demo-controlnet-human-pose",
    "href": "src/01/exploring_genai.html#demo-controlnet-human-pose",
    "title": "Exploring Generative AI Models",
    "section": "Demo: ControlNet Human Pose",
    "text": "Demo: ControlNet Human Pose\nControlNet Human Pose.ipynb",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#controlnet-2",
    "href": "src/01/exploring_genai.html#controlnet-2",
    "title": "Exploring Generative AI Models",
    "section": "ControlNet",
    "text": "ControlNet\nExamples of conditioning types:\n\nHuman pose (OpenPose) - skeleton/keypoint detection\nQR codes - codes blended into images\nCanny edges - line drawings and edge detection\nDepth maps - 3D structure information\nNormal maps - surface orientation\nSemantic segmentation - labeled regions\nScribbles - rough user drawings",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#hugging-face-image-models",
    "href": "src/01/exploring_genai.html#hugging-face-image-models",
    "title": "Exploring Generative AI Models",
    "section": "Hugging Face Image Models",
    "text": "Hugging Face Image Models\n\nSource: https://huggingface.co/models",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#replicate",
    "href": "src/01/exploring_genai.html#replicate",
    "title": "Exploring Generative AI Models",
    "section": "Replicate",
    "text": "Replicate\n\nSource: https://replicate.com",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#comfyui",
    "href": "src/01/exploring_genai.html#comfyui",
    "title": "Exploring Generative AI Models",
    "section": "ComfyUI",
    "text": "ComfyUI\n\nSource: https://comfy.org",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/01/exploring_genai.html#resources-3",
    "href": "src/01/exploring_genai.html#resources-3",
    "title": "Exploring Generative AI Models",
    "section": "Resources",
    "text": "Resources\n\nThis slide deck, resources, links, everything:\n\nhttps://simonguest.github.io/CSP-400\n(I‚Äôll also post to the GAM-400 and CSP-300/400 channels)",
    "crumbs": [
      "Lecture: Exploring Generative AI Models",
      "Slides"
    ]
  },
  {
    "objectID": "src/networking/http3.html",
    "href": "src/networking/http3.html",
    "title": "HTTP/3",
    "section": "",
    "text": "HTTP/3 is the latest major revision of the Hypertext Transfer Protocol, designed to make web communication faster, more reliable, and better suited for modern network conditions. Unlike HTTP/1.1 and HTTP/2, which are built on top of TCP, HTTP/3 runs over¬†QUIC, a transport protocol based on UDP. By leveraging QUIC, HTTP/3 benefits from features such as built-in encryption (TLS 1.3), multiplexed streams without head-of-line blocking, reduced connection setup time, and graceful handling of network changes (e.g., switching from WiFi to cellular).\nHTTP/3 is increasingly used by browsers, CDNs, and backend services to improve page load times, streaming performance, and resilience to packet loss in high-latency networks.\n\n\n\n\n\nflowchart LR\n    subgraph Client[\"Client (e.g., Browser, Mobile App)\"]\n        C1[HTTP/3 Stack]\n        C2[Application Logic]\n        C2 --&gt; C1\n    end\n\n    subgraph Server[\"Server\"]\n        S1[HTTP/3-capable Web Server / CDN]\n        S2[Application Logic / Handlers]\n        S3[Database / External APIs]\n        S2 --&gt; S3\n        S1 --&gt; S2\n    end\n\n    C1 &lt;--&gt; |\"QUIC (UDP-based) Transport\"| S1\n\n    %% Styling\n    style Client fill:#f0f9ff,stroke:#0369a1,stroke-width:2px,rounded-corners:5px\n    style Server fill:#fef9c3,stroke:#ca8a04,stroke-width:2px,rounded-corners:5px\n\n\n\n\n\n\n\n\n\nThe most widely used Python library for HTTP/3 and QUIC is¬†aioquic.\n\nIt‚Äôs an implementation of QUIC and HTTP/3 built on top of¬†asyncio.\nSupports client and server roles, TLS 1.3, and custom protocol experimentation.\nUseful for both low-level QUIC experiments and HTTP/3 integration with web servers.\n\nOther frameworks are beginning to add¬†HTTP/3 support¬†(e.g., Hypercorn or uvicorn with custom workers), but¬†aioquic¬†remains the go-to library today.\n\n\n\n\n\nEcosystem Maturity¬†‚Äì HTTP/3 adoption is growing, but not all clients, servers, proxies, or CDNs support it yet.\nUDP Dependency¬†‚Äì Some networks (corporate firewalls, older routers) may block or throttle UDP traffic, limiting HTTP/3‚Äôs performance benefits.\nResource Usage¬†‚Äì Running on top of UDP requires more aggressive congestion control and buffer handling, which may make server-side scaling more complex.\nDebugging & Monitoring¬†‚Äì Tools for packet inspection and debugging are less mature than for TCP/HTTP/2.\nFallback Complexity¬†‚Äì Many servers and browsers negotiate between HTTP/1.1, HTTP/2, and HTTP/3, which adds operational complexity.\nNot Always Faster¬†‚Äì For very stable, low-latency connections, HTTP/2 may perform competitively; HTTP/3 shines in lossy or high-latency links.",
    "crumbs": [
      "Networking Protocols",
      "HTTP/3"
    ]
  },
  {
    "objectID": "src/networking/http3.html#python-sdk",
    "href": "src/networking/http3.html#python-sdk",
    "title": "HTTP/3",
    "section": "",
    "text": "The most widely used Python library for HTTP/3 and QUIC is¬†aioquic.\n\nIt‚Äôs an implementation of QUIC and HTTP/3 built on top of¬†asyncio.\nSupports client and server roles, TLS 1.3, and custom protocol experimentation.\nUseful for both low-level QUIC experiments and HTTP/3 integration with web servers.\n\nOther frameworks are beginning to add¬†HTTP/3 support¬†(e.g., Hypercorn or uvicorn with custom workers), but¬†aioquic¬†remains the go-to library today.",
    "crumbs": [
      "Networking Protocols",
      "HTTP/3"
    ]
  },
  {
    "objectID": "src/networking/http3.html#limitations",
    "href": "src/networking/http3.html#limitations",
    "title": "HTTP/3",
    "section": "",
    "text": "Ecosystem Maturity¬†‚Äì HTTP/3 adoption is growing, but not all clients, servers, proxies, or CDNs support it yet.\nUDP Dependency¬†‚Äì Some networks (corporate firewalls, older routers) may block or throttle UDP traffic, limiting HTTP/3‚Äôs performance benefits.\nResource Usage¬†‚Äì Running on top of UDP requires more aggressive congestion control and buffer handling, which may make server-side scaling more complex.\nDebugging & Monitoring¬†‚Äì Tools for packet inspection and debugging are less mature than for TCP/HTTP/2.\nFallback Complexity¬†‚Äì Many servers and browsers negotiate between HTTP/1.1, HTTP/2, and HTTP/3, which adds operational complexity.\nNot Always Faster¬†‚Äì For very stable, low-latency connections, HTTP/2 may perform competitively; HTTP/3 shines in lossy or high-latency links.",
    "crumbs": [
      "Networking Protocols",
      "HTTP/3"
    ]
  },
  {
    "objectID": "src/networking/webrtc.html",
    "href": "src/networking/webrtc.html",
    "title": "WebRTC",
    "section": "",
    "text": "WebRTC (Web Real-Time Communication) is a set of APIs and protocols that enables direct peer-to-peer communication between clients (such as web browsers or mobile apps) without routing all traffic through a central server. It supports real-time transfer of audio, video, and arbitrary data, making it ideal for video conferencing, voice calls, live streaming, and real-time file sharing. Unlike WebSockets, which always rely on a continuous connection with a server, WebRTC establishes a direct connection between peers, often requiring signaling servers and STUN/TURN servers to help with connection setup and NAT/firewall traversal.\n\n\n\n\n\nflowchart LR\n    subgraph ClientA[\"Client A (e.g., Browser, Mobile App)\"]\n        A1[WebRTC API]\n        A2[App Logic]\n        A2 --&gt; A1\n    end\n\n    subgraph ClientB[\"Client B (e.g., Browser, Mobile App)\"]\n        B1[WebRTC API]\n        B2[App Logic]\n        B2 --&gt; B1\n    end\n\n    subgraph Signaling[\"Signaling Server\"]\n        S1[Exchange Session Descriptions & ICE Candidates]\n    end\n\n    subgraph STUN_TURN[\"STUN / TURN Servers\"]\n        N1[Helps with NAT Traversal / Relay Media if Needed]\n    end\n\n    %% Corrected edges (no parentheses or slashes in text)\n    A1 &lt;--&gt; |\"Signaling over WebSocket or HTTP\"| S1\n    B1 &lt;--&gt; |\"Signaling over WebSocket or HTTP\"| S1\n    A1 &lt;--&gt; |\"Peer-to-Peer Media & Data\"| B1\n    A1 &lt;--&gt; N1\n    B1 &lt;--&gt; N1\n\n    %% Styling\n    style ClientA fill:#f0f9ff,stroke:#0369a1,stroke-width:2px,rounded-corners:5px\n    style ClientB fill:#ede9fe,stroke:#6d28d9,stroke-width:2px,rounded-corners:5px\n    style Signaling fill:#fef9c3,stroke:#ca8a04,stroke-width:2px,rounded-corners:5px\n    style STUN_TURN fill:#dcfce7,stroke:#15803d,stroke-width:2px,rounded-corners:5px",
    "crumbs": [
      "Networking Protocols",
      "WebRTC"
    ]
  },
  {
    "objectID": "src/networking/webrtc.html#limitations",
    "href": "src/networking/webrtc.html#limitations",
    "title": "WebRTC",
    "section": "Limitations",
    "text": "Limitations\n\nSignaling Dependency¬†‚Äì WebRTC itself does not specify how peers discover and exchange connection information; a separate signaling mechanism (e.g., WebSockets or HTTP) is always required.\nNAT and Firewall Traversal¬†‚Äì Direct peer-to-peer connections can fail in restrictive network environments, making STUN/TURN servers necessary, which adds complexity and cost.\nScalability¬†‚Äì Peer-to-peer topologies work well for small groups, but scaling to large calls (e.g., many participants in a video conference) typically requires media servers (SFU/MCU).\nImplementation Complexity¬†‚Äì WebRTC supports multiple codecs, transport mechanisms, and NAT traversal strategies, which can be complex to configure correctly.\nResource Intensive¬†‚Äì Real-time audio and video encoding/decoding can be CPU and bandwidth heavy, especially on lower-powered devices.\nLimited Server Control¬†‚Äì Since traffic often flows directly between peers, servers have less visibility and control compared to client-server communication models like WebSockets.",
    "crumbs": [
      "Networking Protocols",
      "WebRTC"
    ]
  }
]